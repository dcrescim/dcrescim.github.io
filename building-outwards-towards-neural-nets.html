<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Building outwards towards Neural Nets</title>
  <meta name="author" content="Dan">



  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link href="/favicon.png" rel="icon">
  <link href="/theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">
  <script src="/theme/js/modernizr-2.0.js"></script>
  <script src="/theme/js/ender.js"></script>
  <script src="/theme/js/octopress.js" type="text/javascript"></script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
    });
  </script>

  <script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>


  <link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
</head>

<body>
  <header role="banner"><hgroup>
  <h1><a href="/">Test</a></h1>
</hgroup></header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/" rel="subscribe-rss">RSS</a></li>
</ul>

<!-- TODO: add search here
<form action="" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
-->

<ul class="main-navigation">
    <!-- TODO: add categories here? -->
</ul></nav>
  <div id="main">
    <div id="content">
<div>
  <article class="hentry" role="article">
<header>
      <h1 class="entry-title">Building outwards towards Neural Nets</h1>
      <p class="meta"><time datetime="2014-03-25T00:00:00" pubdate>Tue 25 March 2014</time></p>
</header>

  <div class="entry-content"><p>This post assumes you have read and understood the post on <a class="reference external" href="/linear-regression.html">linear regression</a>.</p>
<div class="section" id="neural-nets">
<h2>Neural Nets</h2>
<p>Today we are going to build the framework for neural nets. Well what are neural nets? Here is a picture of one</p>
<img alt="2-Layer Neural Net" src="theme/images/2-layer-neural.png" style="height: 400px;" />
<p>These are not the only type of neural nets. You can connect nodes to themselves, and have edges that go &quot;backwords&quot;. But for right now, I'm restricting my attention to neural nets where every layer is fully connected to the layer right after it. Here is another simpler neural net, that is only 1 layer deep.</p>
<img alt="1-Layer Neural Net" src="theme/images/1-layer-neural.png" style="height: 400px;" />
<p>Ok, but what does this simple diagram represent? When people talk about neural nets they usually have some input values which they assign to the nodes on the very left. Then these values are passed along each edge to the nodes on the right. In transit though, these values are multiplied by a unique edge value (present in all edges). Afterwards. all of the inputs coming into a node on the right are summed up. Then there is an optional function that is applied to the values in the nodes on the right.</p>
</div>
<div class="section" id="matrices">
<h2>Matrices</h2>
<p>If you are anything like me, and you just read the previous paragraph, then you are a white hot ball of rage right now. The reason why you are gnashing your teeth and breaking things, is because you realize this is a colossally verbose and stupidly complex way of multiplying your input by a matrix (and then optionally applying a function). Here is that same picture again, but with the weights in the edges organized in matrix form.</p>
<img alt="Layer as Matrix" src="theme/images/layer-matrix.png" style="height: 400px;" />
<p>We see that the input row is $x_{row}$, and the output is just $x_{row} \cdot W$ (with optional function). In words, we see that one layer of a neural net is a matrix and a function.</p>
<p>I want you to pause and really ponder the differences between the last two images. The relationship between these two drawings is the relationship between &quot;The Cloud&quot; and &quot;A Server Somewhere&quot;. Technically, they are the same thing, but one <strong>sounds</strong> way sexier. You can write papers about &quot;The Cloud&quot;. Very rarely are papers written about &quot;A Server Somewhere&quot;. Those papers don't get read, and they don't get grants. So on an intellectual level you can understand maybe how one image became temporarily more popular, but it is impossible to stop that small voice in the back of your head from shouting</p>
<blockquote>
Please for the love of all that is green on this Earth, use matrices. It's simpler.</blockquote>
</div>
<div class="section" id="networks">
<h2>Networks</h2>
<p>After creating such a creature, people began stringing them together to make more complicated networks like this. This is simply another layer added after the first layer, to make a larger network.</p>
<img alt="2-Layer Neural Net" src="theme/images/2-layer-neural.png" style="height: 400px;" />
<p>Correspondingly, my image devoid of riff-raff is</p>
<img alt="2-Layer Matrix View" src="theme/images/2-layer-matrix.png" style="height: 500px;" />
<p>But that is just one input. We can imagine passing in a bunch of inputs at once to create something like this.</p>
<img alt="Larger Models" src="theme/images/LargerModels.png" />
<p>So we see pictorially that the model we are creating for neural nets is just a generalization of the models that we have created for linear regression and logistic classification. We just have more internal matrices now (and a wider variety of functions to apply after the dot). Well, I'll be darned!</p>
<blockquote>
Like every gradient descent problem, the goal is to update the internal matrices to make Y close to T.</blockquote>
<p>So the next question becomes, how do we update our internal matrices? We can do what we have always done which is just compute the partial derivatives of our error function with respect to every weight, and just walk down in error. That would definitely work. But what about this new-fangled approach called &quot;backpropagation&quot; that sounds so sexy and is all abuzz right now. Well I'm about to let the cat out of the bag</p>
<blockquote>
Backpropagation IS the computation of the partial derivatives</blockquote>
</div>
<div class="section" id="backpropagation">
<h2>Backpropagation</h2>
<p>That's right folks! You heard it hear first. They are one in the same.  This brings me to one of the things that really bothers me about neural nets/backpropagation, namely that they are shrouded in mysticism. Every time I look up backpropagation, I get a series of arcane steps that are equivalent to medieval alchemy. Here is a screenshot of Andrew Ng's lecture as he begins to describe how the backpropagation algorithm works, linked <a class="reference external" href="http://www.youtube.com/watch?v=wmfpS5fKFeY">here</a> (it's on youtube).</p>
<img alt="Backprop" src="theme/images/backprop_ng2.png" style="height: 400px;" />
<p>The problems here begin with a wildly new notation to represent every node. Then there are these &quot;rules&quot; to compute the &quot;error&quot; of previous nodes. It's unclear where these rules came from. It's also unclear what exactly these &quot;errors&quot; represent (in my derivation, you realize that they are partial derivatives). My favorite part is at moment 4:30, where magically some equations leap onto the screen. Awesome! What I love most in the world are equations with no context!</p>
<p>Look, I get it. I understand that Andrew Ng is trying to sidestep the math, so that we (the reader) can write this mystery code and move on with our day. But <strong>ideas are only understood as an extension to our own thinking</strong>. Telling someone an answer is not knowledge. Telling someone a story, wherein they can create an answer, is knowledge.</p>
<blockquote>
Teachers that tell you the end of a story, are a dime a dozen. Teachers that weave a narrative so compelling that you write the end of the story, are in another league entirely.</blockquote>
<p>All that being said, I think Andrew Ng is a god. Even while I poke fun at his rendition of neural nets, I have benefited greatly from the thousands of things that he has taught me.</p>
<p>His rendition has information. That much can't be said for &quot;backpropagation&quot; as explained by ...</p>
</div>
<div class="section" id="wikipedia">
<h2>Wikipedia</h2>
<p>God have mercy on your soul if you ever try to read the pseudocode for <a class="reference external" href="http://en.wikipedia.org/wiki/Backpropagation">backpropagation</a> on wikipedia. Nothing could be more of an abstraction. I'm surprised they didn't go with something like</p>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">backpropagate</span><span class="p">():</span>
  <span class="n">compute</span> <span class="n">delta</span> <span class="k">for</span> <span class="nb">all</span> <span class="n">weights</span>
  <span class="n">update</span> <span class="n">weights</span>
</pre></div>
<p>I mean, give me a break. How is any newcomer going to learn a damn thing from that &quot;pseudocode&quot;. Even looking at the real code is about as clear as mud. This snippet is taken from the python code linked at the bottom of wikipedia.</p>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">backPropagate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">):</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">no</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;wrong number of target values&#39;</span><span class="p">)</span>

  <span class="c"># calculate error terms for output</span>
  <span class="n">output_deltas</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">no</span>
  <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">no</span><span class="p">):</span>
      <span class="n">error</span> <span class="o">=</span> <span class="n">targets</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">ao</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
      <span class="n">output_deltas</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">dsigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ao</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="o">*</span> <span class="n">error</span>

  <span class="c"># calculate error terms for hidden</span>
  <span class="n">hidden_deltas</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">nh</span>
  <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nh</span><span class="p">):</span>
      <span class="n">error</span> <span class="o">=</span> <span class="mf">0.0</span>
      <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">no</span><span class="p">):</span>
          <span class="n">error</span> <span class="o">=</span> <span class="n">error</span> <span class="o">+</span> <span class="n">output_deltas</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">wo</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>
      <span class="n">hidden_deltas</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">dsigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ah</span><span class="p">[</span><span class="n">j</span><span class="p">])</span> <span class="o">*</span> <span class="n">error</span>

  <span class="c"># update output weights</span>
  <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nh</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">no</span><span class="p">):</span>
          <span class="n">change</span> <span class="o">=</span> <span class="n">output_deltas</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">ah</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">wo</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wo</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">N</span><span class="o">*</span><span class="n">change</span> <span class="o">+</span> <span class="n">M</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">co</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">co</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">change</span>
          <span class="c">#print N*change, M*self.co[j][k]</span>

  <span class="c"># update input weights</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ni</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nh</span><span class="p">):</span>
          <span class="n">change</span> <span class="o">=</span> <span class="n">hidden_deltas</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">ai</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">wi</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wi</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">N</span><span class="o">*</span><span class="n">change</span> <span class="o">+</span> <span class="n">M</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">ci</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">ci</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">change</span>

  <span class="c"># calculate error</span>
  <span class="n">error</span> <span class="o">=</span> <span class="mf">0.0</span>
  <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">targets</span><span class="p">)):</span>
      <span class="n">error</span> <span class="o">=</span> <span class="n">error</span> <span class="o">+</span> <span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="n">targets</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">ao</span><span class="p">[</span><span class="n">k</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span>
  <span class="k">return</span> <span class="n">error</span>
</pre></div>
<p>Now what in the hell is going on here? Lots of things are getting calculated, but there is obviously a breakdown in <strong>meaning</strong>. What are we computing? And why are we computing it? And is this the best way to do that? This code is warning code about the dangers of not using matrices. I mean just look at it. The author has decided to do matrix operations <strong>without using matrix operations</strong>.</p>
</div>
<div class="section" id="the-beginning">
<h2>The beginning</h2>
<blockquote>
The harder way is easier -- Zed Shaw</blockquote>
<p>Sit down. Take out your pencil and take the partial derivative. Let's derive backpropagation. Someone else did it, and so can you.</p>
</div>
<div class="section" id="derivative">
<h2>Derivative</h2>
<p>With two rules you are going to rederive the &quot;backpropagation&quot; algorithm, also known as the &quot;compute partial derivatives&quot; algorithm. You already know one of them. This first rule I call <strong>Partial North</strong>, and you derived it way back in the post on <a class="reference external" href="/linear-regression.html">linear regression</a>.</p>
<p>Basically, you had a model like this.</p>
<img alt="Linear Regression" src="theme/images/LinReg.png" style="height: 400px;" />
<p>From this image, you were able to compute $\dfrac{dJ}{dY}$. This quantity is a matrix of size $Y$, where the element at index $(i,j)$ is $\dfrac{dJ}{dy_{ij}}$. I like to think of this matrix as the partials <strong>in their correct place</strong>. If you are unfamiliar with this notation please read the <a class="reference external" href="/linear-regression.html">linear regression</a> post.</p>
<p>From that, you figured out that you could compute $\dfrac{dJ}{dW}$, which is exactly the quantity you needed in order to update your weights.</p>
</div>
<div class="section" id="partial-north">
<h2>Partial North</h2>
<p>$$\dfrac{dJ}{dW} = X^{T} \cdot \dfrac{dJ}{dY}$$</p>
<p>Now this <strong>Partial North</strong> calculation implies that you are not applying a function after you dot $X$ and $W$ (which you are not in linear regression). But for more interesting models (like logistic classification and neural nets), you are going to need to factor that into your equation. Don't worry, we do the more general case down below. But before we do the general case, let's do the easy case (assume no function), but let's go west this time.</p>
<p>And so we create the ...</p>
</div>
<div class="section" id="partial-west">
<h2>Partial West</h2>
<p>To be more specific, you really want to compute the quantity known as $\dfrac{dJ}{dX}$. In a linear regression, this quantity doesn't really make sense, as you can't change your input values. But with larger models, we <strong>can</strong> vary those internal matrices $X_{k}$ implicitly by changing weights of our internal matrices. Take for example the fact that we can change the values in $X_{0}$, if we vary the values in $W_{1}$ in the image below</p>
<img alt="Larger Models" src="theme/images/LargerModels.png" />
<p>Now let's stare at our original linear model picture and compute $\dfrac{dJ}{dX}$ (pretending for the moment that we can actually change $x_{ab}$). Once we compute the partial west, we can recursively use our <strong>Partial North</strong> and <strong>Partial West</strong> to generate the derivatives for our weight matrices.</p>
<img alt="Linear Regression" src="theme/images/LinReg.png" style="height: 400px;" />
</div>
<div class="section" id="id4">
<h2>Derivative</h2>
<p>What do we know?</p>
<p>$$J = \dfrac{1}{2m} \sum_{i,j} (y_{ij} - t_{ij})^{2} $$</p>
<p>$$\dfrac{dJ}{dx_{ab}} = \dfrac{1}{m} \sum_{i,j} \dfrac{dJ}{dy_{ij}} * \dfrac{dy_{ij}}{dx_{ab}} = \dfrac{1}{m} \sum_{i,j} (y_{ij} - t_{ij})\dfrac{dy_{ij}}{dx_{ab}}$$</p>
<p>To get a sense of that second term consider the following pictures. This analysis is very similar to that done with linear regression.</p>
<img alt="Partial in x" src="theme/images/partial_y_x.png" />
<p>Given that $X \cdot W = Y$, consider a random element $y_{ij}$, and some equally random element of the $X$ matrix $x_{ab}$. How is $y_{ij}$ changed if we vary $x_{ab}$? It's pretty easy to see that changing $x_{ab}$ has no effect on $y_{ij}$, if $i \neq a$, because they are not in the same row.</p>
<img alt="Partial in x" src="theme/images/partial_y_x_row.png" />
<p>If they are in the same row, it is not hard to see that</p>
<p>$$\dfrac{dy_{ij}}{dx_{ab}} = w_{bj}$$
if $a = i$, otherwise it is 0.</p>
<p>So the expression becomes</p>
<p>$$\dfrac{dJ}{dx_{ab}} = \dfrac{1}{m} \sum_{j} (y_{aj} - t_{aj})w_{bj}$$</p>
<p>Because we can set $i = a$. Hmmm... So how can we simplify this expression? Let's consider one element, say $x_{23}$, and imagine what we are summing up to compute that value. Well we can stare at this <strong>awesome</strong> <a class="reference external" href="http://codepen.io/dcrmls/pen/JjgrI">visualization</a> of that calculation for value $x_{23}$. And if you glare at this for a while, take a swig of strong cognac, and tilt our head to the side, it might become clear to you how to proceed.</p>
<p>What we really want is to dot one row in $\dfrac{dJ}{dY}$ with another row in $W$. So if we take that one row in $\dfrac{dJ}{dY}$ and dot it with the <strong>transpose</strong> of that row in $W$, we get exactly what we want.</p>
<p>$$\dfrac{dJ}{dx_{ab}} = \dfrac{1}{m} (Y_{a\_} - T_{a\_}) \cdot W_{b\_}^{T}$$</p>
<p>Collecting those terms into a big 'ol matrix we find that</p>
<p>$$\dfrac{dJ}{dX} = \dfrac{1}{m} (Y-T) \cdot W^{T}$$</p>
<p>or more concisely</p>
<p>$$\dfrac{dJ}{dX} = \dfrac{dJ}{dY} \cdot W^{T}$$</p>
<p>This is what I call the <strong>West Partial</strong></p>
</div>
<div class="section" id="west-partial">
<h2>West Partial</h2>
<p>$$\dfrac{dJ}{dX} = \dfrac{dJ}{dY} \cdot W^{T}$$</p>
<p>Woah! Pretty sweet! But these two rules both assume that we are not applying a function after we dot $X$ and $W$. How can we make this generalize?</p>
<p>Well after thinking about the chain rule, and looking over our analysis for linear regression and logistic classification, we realize that every element in $\dfrac{dJ}{dY}$ needs to be multiplied by the derivative $f^{'}$ evaluated at our linear output.</p>
<p>If this seems like a leap, check out the derivation of the logistic classification algorithm, and look at what we multiply our $\dfrac{dJ}{dY}$ by. The link is <a class="reference external" href="/building-outward-toward-classification.html">right here</a></p>
</div>
<div class="section" id="generalization">
<h2>Generalization</h2>
<p>Here we are assuming $Y = f(X \cdot W)$</p>
<div class="section" id="generalized-north-partial">
<h3>Generalized North Partial</h3>
<p>$$\dfrac{dJ}{dW} = X^{T} \cdot (\dfrac{dJ}{dY} * f^{'}(X \cdot W))$$</p>
</div>
<div class="section" id="generalized-west-partial">
<h3>Generalized West Partial</h3>
<p>$$\dfrac{dJ}{dX} = (\dfrac{dJ}{dY} * f^{'}(X \cdot W)) \cdot W^{T}$$</p>
<p>Note that $\cdot$ is the dot product, and $*$ is an element-wise multiplication. Big difference.</p>
<p>Just for future reference, when I write $\dfrac{A}{B}$ I mean the element-wise division of those two matrices. If I wanted to multiply $A$ by the inverse of $B$, I would write it like this $A \cdot B^{-1}$.</p>
</div>
</div>
<div class="section" id="nice">
<h2>Nice!</h2>
<p>And there you have it. The keys to the hovercar. From here, we see that we can recursively use these two rules to take steps west and north. This will create the partial derivative of whatever internal matrix, $W$, that we need. So armed with our two rules, we can now calculate just about anything for this class of generalized models.</p>
<p>Now it might not be obvious just how awesome these rules are just yet. So to make that clear and show how to use them, let's recreate all of the math for all of the models that we have build up to this point.</p>
</div>
<div class="section" id="linear-regression-with-squared-error">
<h2>Linear Regression with Squared Error</h2>
<p>We know that
$$\dfrac{dJ}{dY} = \dfrac{1}{m} (Y-T)$$</p>
<p>$$\dfrac{dJ}{dW} = X^{T} \cdot \dfrac{dJ}{dY} = \dfrac{1}{m} X^{T} \cdot (Y - T)$$</p>
</div>
<div class="section" id="logistic-classification-with-cross-entropy-error">
<h2>Logistic Classification with Cross-Entropy Error</h2>
<p>What is $\dfrac{dJ}{dY}$ for cross entropy error?</p>
<p>In the logistic regression post, we derived that</p>
<p>$$\dfrac{dCE(y_{ij}, t_{ij})}{dy_{ij}} = \dfrac{y_{ij} - t_{ij}}{y_{ij}(1-y_{ij})}$$</p>
<p>So every element of $\dfrac{dJ}{dY}$, must be that, so then</p>
<p>$$\dfrac{dJ}{dY} = \dfrac{1}{m} \dfrac{Y-T}{Y(1-Y)}$$</p>
<p>Element-wise division above.</p>
<p>So then we see that</p>
<p>$$\dfrac{dJ}{dW} = X^{T} \cdot \left(\dfrac{dJ}{dY} * Sig^{'}(X \cdot W) \right) = X^{T} \cdot \left( \dfrac{1}{m} \dfrac{Y-T}{Y(1-Y)} * Y(1-Y) \right) = \dfrac{1}{m} X^T \cdot (Y-T)$$</p>
<p>Well that was certainly faster.</p>
</div>
<div class="section" id="thoughts">
<h2>Thoughts</h2>
<p>The main benefit of all of this stuff is that the math suddenly becomes plug-and-playable. If you wanted to consider a totally different error function, then all you need to do is create the corresponding $\dfrac{dJ}{dY}$, and the rest holds. If you wanted to try a different function $F$ after one of your dots ($X$ and $W$), then you just need to compute $F^{'}(X \cdot W)$, and everything else holds.</p>
</div>
<div class="section" id="neural-net-2-layers">
<h2>Neural Net 2 Layers</h2>
<p>Let's say that this Neural net has a sigmoid function attached to both of it's layers. $f = Sig$ in the picture below.</p>
<img alt="3 Layer Neural Net" src="theme/images/3-LayerNeuralNetv2.png" />
<p>Let's start walking our error back.</p>
<div class="section" id="partials-end">
<h3>Partials End</h3>
<p>$$\dfrac{dJ}{dY} = \dfrac{Y - T}{Y(1-Y)}$$</p>
</div>
<div class="section" id="id5">
<h3>Partial North</h3>
<p>$$\dfrac{dJ}{dW_0} = X_0^{T} \cdot \left(\dfrac{dJ}{dY} * Sig^{'}(X_0 \cdot W_0) \right) = \dfrac{1}{m} X_0^{T} \cdot (Y - T)$$</p>
</div>
<div class="section" id="id6">
<h3>Partial West</h3>
<p>$$\dfrac{dJ}{dX_1} = \left(\dfrac{dJ}{dY} * Sig^{'}(X_0 \cdot W_0) \right) \cdot W^{T} = \dfrac{1}{m} (Y - T) \cdot W_0^{T}$$</p>
</div>
<div class="section" id="id7">
<h3>Partial North</h3>
<p>$$\dfrac{dJ}{dW_1} = X_1^{T} \cdot \left(\dfrac{dJ}{dX_1} * Sig^{'}(X_1 \cdot W_1) \right)
= \dfrac{1}{m} X_1^{T} \cdot \left((Y-T) \cdot W_0^{T} * X_0(1-X_0)\right)$$</p>
<p>Yes, that last expression is kinda big and ugly. Now, I want you to imagine doing that calculation <strong>without matrices</strong>. While you may never do this by hand again, it is nice to know that you are basically doing calculus through matrices, and that the rules for propagating partials are ones that you have created. Once you code this up, the alternation of the north and west partials will calculate the update to your internal matrices.</p>
</div>
</div>
<div class="section" id="time-to-code">
<h2>Time to Code</h2>
<p>Now this is the fun part. It's time to write the Neural Net. I'll see you next week.</p>
<p>For me:
explain partial north, partial west</p>
</div>
</div>
    <footer>
<p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">Dan Crescimanno</span>
  </span>
<time datetime="2014-03-25T00:00:00" pubdate>Tue 25 March 2014</time></p><div class="sharing">
</div>    </footer>
  </article>

</div>
<aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="/building-outwards-towards-neural-nets.html">Building outwards towards Neural Nets</a>
      </li>
      <li class="post">
          <a href="/logistic-classification-code.html">Logistic Classification Code</a>
      </li>
      <li class="post">
          <a href="/building-outward-toward-classification.html">Building outward toward Classification</a>
      </li>
      <li class="post">
          <a href="/linear-regression-code-pt-2.html">Linear Regression Code pt 2</a>
      </li>
      <li class="post">
          <a href="/linear-regression-code-pt-1.html">Linear Regression Code pt 1</a>
      </li>
    </ul>
  </section>
  <section>
      
    <h1>Categories</h1>
    <ul id="recent_posts">
        <li><a href="/category/python-math.html">python, math</a></li>
    </ul>
  </section>
 

  <section>
  <h1>Tags</h1>
  </section>


    <section>
        <h1>Social</h1>
        <ul>
            <li><a href="/" type="application/rss+xml" rel="alternate">RSS</a></li>
            <li><a href="#" target="_blank">You can add links in your config file</a></li>
            <li><a href="#" target="_blank">Another social link</a></li>
        </ul>
    </section>
    <section>
        <h1>Blogroll</h1>
        <ul>
            <li><a href="http://getpelican.com/" target="_blank">Pelican</a></li>
            <li><a href="http://python.org/" target="_blank">Python.org</a></li>
            <li><a href="http://jinja.pocoo.org/" target="_blank">Jinja2</a></li>
            <li><a href="#" target="_blank">You can modify those links in your config file</a></li>
        </ul>
    </section>

</aside><aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="/building-outwards-towards-neural-nets.html">Building outwards towards Neural Nets</a>
      </li>
      <li class="post">
          <a href="/logistic-classification-code.html">Logistic Classification Code</a>
      </li>
      <li class="post">
          <a href="/building-outward-toward-classification.html">Building outward toward Classification</a>
      </li>
      <li class="post">
          <a href="/linear-regression-code-pt-2.html">Linear Regression Code pt 2</a>
      </li>
      <li class="post">
          <a href="/linear-regression-code-pt-1.html">Linear Regression Code pt 1</a>
      </li>
    </ul>
  </section>
  <section>
      
    <h1>Categories</h1>
    <ul id="recent_posts">
        <li><a href="/category/python-math.html">python, math</a></li>
    </ul>
  </section>
 

  <section>
  <h1>Tags</h1>
  </section>


    <section>
        <h1>Social</h1>
        <ul>
            <li><a href="/" type="application/rss+xml" rel="alternate">RSS</a></li>
            <li><a href="#" target="_blank">You can add links in your config file</a></li>
            <li><a href="#" target="_blank">Another social link</a></li>
        </ul>
    </section>
    <section>
        <h1>Blogroll</h1>
        <ul>
            <li><a href="http://getpelican.com/" target="_blank">Pelican</a></li>
            <li><a href="http://python.org/" target="_blank">Python.org</a></li>
            <li><a href="http://jinja.pocoo.org/" target="_blank">Jinja2</a></li>
            <li><a href="#" target="_blank">You can modify those links in your config file</a></li>
        </ul>
    </section>

</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2013 - Dan -
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
</body>
</html>