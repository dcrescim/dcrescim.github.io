<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Linear Regression Code pt 1</title>
  <meta name="author" content="Dan Crescimanno">



  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link href="/favicon.png" rel="icon">
  <link href="/theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">
  <script src="/theme/js/modernizr-2.0.js"></script>
  <script src="/theme/js/ender.js"></script>
  <script src="/theme/js/octopress.js" type="text/javascript"></script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
    });
  </script>

  <script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>


  <link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
</head>

<body>
  <header role="banner"><hgroup>
  <h1><a href="/">What I cannot create, I do not understand</a></h1>
</hgroup></header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/" rel="subscribe-rss">RSS</a></li>
</ul>

<!-- TODO: add search here
<form action="" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
-->

<ul class="main-navigation">
    <!-- TODO: add categories here? -->
</ul></nav>
  <div id="main">
    <div id="content">
<div>
  <article class="hentry" role="article">
<header>
      <h1 class="entry-title">Linear Regression Code pt 1</h1>
      <p class="meta"><time datetime="2014-01-13T00:00:00" pubdate>Mon 13 January 2014</time></p>
</header>

  <div class="entry-content"><div class="section" id="last-time">
<h2>Last Time</h2>
<p>Aside from learning a new visual for matrix multiplication, we basically learned one major thing last time</p>
<p>1. If our total error for our linear regression was
$$J = \dfrac{1}{2m} \sum_{i,j} (y_{ij} - t_{ij})^{2} $$</p>
<p>then the update to our weight matrix can be written as</p>
<p>$$W_{update} = -\alpha \dfrac{dJ}{dW} = -\dfrac{\alpha}{m}X^{T}\cdot(Y-T)$$</p>
<p>where $\alpha$ is the learning rate (read small constant). If you haven't read the previous post, I suggest you read <a class="reference external" href="/linear-regression.html">it</a> .</p>
</div>
<div class="section" id="breakdown">
<h2>Breakdown</h2>
<p>So what do we need in our linear regression? Well, I want to use it like this.</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">dot</span><span class="p">,</span> <span class="n">transpose</span>

<span class="k">class</span> <span class="nc">LinearRegression</span><span class="p">:</span>

  <span class="c"># Constructor</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">error_func</span><span class="o">=</span><span class="n">SquaredError</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">n_iter</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">):</span>
    <span class="o">...</span>

  <span class="c"># Feeds our input through our model</span>
  <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="o">...</span>

  <span class="c"># Given input, and truth, updates weights, once</span>
  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
    <span class="o">...</span>

  <span class="c"># Updates weights as many times as needed</span>
  <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
    <span class="o">...</span>


<span class="c"># This is how I plan on using it</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">L</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">T</span><span class="p">)</span>
<span class="n">L</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
</pre></div>
<p>So let's take this function by function.</p>
</div>
<div class="section" id="internal-functions">
<h2>Internal Functions</h2>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">error_func</span><span class="o">=</span><span class="n">SquaredError</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">n_iter</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>
  <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
  <span class="bp">self</span><span class="o">.</span><span class="n">error_func</span> <span class="o">=</span> <span class="n">error_func</span>
  <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span> <span class="o">=</span> <span class="n">n_iter</span>
  <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">W</span> <span class="k">if</span> <span class="n">W</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
</pre></div>
<p>Basically, I want to pass in all of the variables that this algorithm needs. We obviously need the error function, the learning rate, and the total number of iterations that we want to use with our gradient descent.</p>
<p>There are other methods that are smarter than just using a total number of iterations. I leave it to you to make this more badass (see the questions below).</p>
<p>The matrix self.W cannot be created until we know it's dimensions. That being said, sometimes I want to pass in a matrix $W$ as a starting point so that I may test my update against another open source implementation that I trust to be correct.</p>
<p>And we only know those dimensions of $W$ once we call <em>fit</em>. We are going to write the <em>fit</em> function right after we sort out the error function.</p>
</div>
<div class="section" id="error-funcs">
<h2>Error Funcs</h2>
<p>Well, what about this SquaredError class? Well, I kinda wanted a class that packaged the squared error function (needed for the <em>forward</em> pass of our algorithm) and the gradient/derivative of the squared error function (needed for the <em>backward</em> pass of our algorithm). Let's write those functions first, and then package 'em together.</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">multiply</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">squared_error</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">T</span><span class="p">):</span>
  <span class="n">m</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mf">1.0</span> <span class="c"># Force into real number</span>
  <span class="n">diff</span> <span class="o">=</span> <span class="n">Y</span> <span class="o">-</span> <span class="n">T</span>
  <span class="n">squares</span> <span class="o">=</span> <span class="n">multiply</span><span class="p">(</span><span class="n">diff</span><span class="p">,</span> <span class="n">diff</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">squares</span><span class="p">)</span><span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">m</span><span class="p">)</span>
</pre></div>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">squared_error_grad</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">T</span><span class="p">):</span>
  <span class="n">rows</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mf">1.0</span><span class="c"># Force into real number</span>
  <span class="n">diff</span> <span class="o">=</span> <span class="n">Y</span><span class="o">-</span><span class="n">T</span>
  <span class="k">return</span> <span class="n">diff</span><span class="o">/</span><span class="n">rows</span>
</pre></div>
<p>So we see that <strong>square_error</strong> represents $J$, and the <strong>squared_error_grad</strong> represents $\dfrac{dJ}{dY}$. Check out the first <a class="reference external" href="/linear-regression.html">post</a>  (under the Quick Aside section) if this notation confuses you.</p>
<p>By all means, pass this pair of error functions around as a tuple or something. I chose to use a class (called SquaredError) with static methods (<strong>func</strong> and <strong>grad</strong>) so that I can type things like</p>
<div class="highlight"><pre><span class="n">SquaredError</span><span class="o">.</span><span class="n">func</span><span class="p">(</span><span class="n">output</span><span class="p">,</span><span class="n">truth</span><span class="p">)</span>
<span class="n">SquaredError</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">output</span><span class="p">,</span><span class="n">truth</span><span class="p">)</span>
</pre></div>
<p>But that is just a little syntactic sugar. Here is my class.</p>
</div>
<div class="section" id="error-class">
<h2>Error Class</h2>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">multiply</span><span class="p">,</span> <span class="nb">sum</span>

<span class="k">class</span> <span class="nc">SquaredError</span><span class="p">:</span>
  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">T</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mf">1.0</span> <span class="c"># Force into real number</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">Y</span> <span class="o">-</span> <span class="n">T</span>
    <span class="n">squares</span> <span class="o">=</span> <span class="n">multiply</span><span class="p">(</span><span class="n">diff</span><span class="p">,</span> <span class="n">diff</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">squares</span><span class="p">)</span><span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">m</span><span class="p">)</span>


  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">T</span><span class="p">):</span>
    <span class="n">rows</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mf">1.0</span><span class="c"># Force into real number</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">Y</span><span class="o">-</span><span class="n">T</span>
    <span class="k">return</span> <span class="n">diff</span><span class="o">/</span><span class="n">rows</span>
</pre></div>
<p>Now let's take a stab at the <strong>fit</strong> method.</p>
</div>
<div class="section" id="fit">
<h2>Fit</h2>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">T</span><span class="p">):</span>
  <span class="c"># If we haven&#39;t created an internal matrix</span>
  <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">init_matrix</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">T</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span><span class="p">):</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
      <span class="k">print</span> <span class="s">&quot;Error: </span><span class="si">%f</span><span class="s">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">T</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">T</span><span class="p">)</span>
</pre></div>
<p>Basically <strong>fit</strong> just iterates, and calls the update function, after first making our internal matrix, $self.W$.</p>
</div>
<div class="section" id="internal-matrix">
<h2>Internal Matrix</h2>
<div class="highlight"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">init_matrix</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">):</span>
  <span class="n">a</span> <span class="o">=</span> <span class="mf">1.0</span><span class="o">/</span><span class="n">rows</span>
  <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span>
    <span class="n">low</span> <span class="o">=</span> <span class="o">-</span><span class="n">a</span><span class="p">,</span>
    <span class="n">high</span> <span class="o">=</span> <span class="n">a</span><span class="p">,</span>
    <span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">)</span>
  <span class="p">))</span>
</pre></div>
<p>We want the weights centered around 0. That being said, there are other way to initialize our internal matrix. I invite you to experiment with alternative methods.</p>
</div>
<div class="section" id="feed-forward">
<h2>Feed-Forward</h2>
<p>Now, let's write the <strong>predict</strong> function</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">dot</span>

<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span>
</pre></div>
<p>That is us feeding our regression the input. Pretty standard. After we have fed our input through the matrix, it is not hard to see how to compute the error.</p>
</div>
<div class="section" id="error-method">
<h2>Error Method</h2>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">error</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
  <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
  <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">error_func</span><span class="o">.</span><span class="n">func</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">T</span><span class="p">)</span>
</pre></div>
<p>Now let's take a stab at <strong>update</strong>. This is where the magic happens.</p>
</div>
<div class="section" id="propagate-back">
<h2>Propagate-Back</h2>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">dot</span><span class="p">,</span> <span class="n">transpose</span>

<span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
  <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
  <span class="n">dJ_dY</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">error_func</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">T</span><span class="p">)</span>
  <span class="n">dJ_dW</span> <span class="o">=</span> <span class="n">dot</span><span class="p">(</span><span class="n">transpose</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">dJ_dY</span><span class="p">)</span>
  <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="o">*</span><span class="n">dJ_dW</span>
</pre></div>
<p>So we first make the output $Y$, and then start walking the partial derivatives back. The first stop on our trip is $\dfrac{dJ}{dY}$, or the gradient applied to (Y,T). Then once we have our partials, we compute $\dfrac{dJ}{dW}$.</p>
<p>But that just updates our parameters once, so we need the <strong>fit</strong> to loop over the number of iterations that we have passed into the constructor (as $n_iter$).</p>
</div>
<div class="section" id="full-code">
<h2>Full code</h2>
<div class="highlight"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">dot</span><span class="p">,</span> <span class="n">transpose</span><span class="p">,</span> <span class="n">multiply</span>

<span class="k">class</span> <span class="nc">SquaredError</span><span class="p">:</span>
  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">T</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mf">1.0</span> <span class="c"># Force into real number</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">Y</span> <span class="o">-</span> <span class="n">T</span>
    <span class="n">squares</span> <span class="o">=</span> <span class="n">multiply</span><span class="p">(</span><span class="n">diff</span><span class="p">,</span> <span class="n">diff</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">squares</span><span class="p">)</span><span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">m</span><span class="p">)</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">T</span><span class="p">):</span>
    <span class="n">rows</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mf">1.0</span><span class="c"># Force into real number</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">Y</span><span class="o">-</span><span class="n">T</span>
    <span class="k">return</span> <span class="n">diff</span><span class="o">/</span><span class="n">rows</span>

<span class="k">class</span> <span class="nc">LinearRegression</span><span class="p">:</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">error_func</span><span class="o">=</span><span class="n">SquaredError</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">n_iter</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">error_func</span> <span class="o">=</span> <span class="n">error_func</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span> <span class="o">=</span> <span class="n">n_iter</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">W</span> <span class="k">if</span> <span class="n">W</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">init_matrix</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">):</span>
    <span class="n">a</span> <span class="o">=</span> <span class="mf">1.0</span><span class="o">/</span><span class="n">rows</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span>
      <span class="n">low</span> <span class="o">=</span> <span class="o">-</span><span class="n">a</span><span class="p">,</span>
      <span class="n">high</span> <span class="o">=</span> <span class="n">a</span><span class="p">,</span>
      <span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">)</span>
    <span class="p">))</span>

  <span class="k">def</span> <span class="nf">error</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">error_func</span><span class="o">.</span><span class="n">func</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">T</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">dJ_dY</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">error_func</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">T</span><span class="p">)</span>
    <span class="n">dJ_dW</span> <span class="o">=</span> <span class="n">dot</span><span class="p">(</span><span class="n">transpose</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">dJ_dY</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="o">*</span><span class="n">dJ_dW</span>


  <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">T</span><span class="p">):</span>
    <span class="c"># If we haven&#39;t created an internal matrix</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">init_matrix</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">T</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span><span class="p">):</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
        <span class="k">print</span> <span class="s">&quot;Error: </span><span class="si">%f</span><span class="s">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">T</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="one-main-issue">
<h2>One Main Issue</h2>
<p>Well this is cute, but there is one big issue here. In this version of our linear_regression, we don't have an intercept term (gasps!). Don't worry, we rectify this in pt2. But the real question is how would you solve this problem?</p>
<p>Before we write that bit up, let's sanity check this implementation by watching our error decrease as we update our matrix $W$.</p>
</div>
<div class="section" id="testing-the-regression">
<h2>Testing the Regression</h2>
<div class="section" id="get-the-data-preprocess-it-and-run-the-model">
<h3>Get The Data, Preprocess it, and Run the model</h3>
<div class="highlight"><pre><span class="c"># Get the Boston Regression Data</span>
<span class="n">boston</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_boston</span><span class="p">()</span>
<span class="n">X</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="n">boston</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">boston</span><span class="o">.</span><span class="n">target</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">T</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>

<span class="c"># Preprocess the X data, by using a scaling each feature independently.</span>
<span class="n">X_scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X_normed</span> <span class="o">=</span> <span class="n">X_scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c"># Create our linear model</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">L</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_normed</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
<span class="k">print</span> <span class="s">&quot;Our Error: </span><span class="si">%f</span><span class="s">&quot;</span> <span class="o">%</span> <span class="n">L</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="n">X_normed</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
</pre></div>
<p>In my linear regression code, I assume that I am dealing with numpy arrays, $X$ and $T$, that have 2 dimensions (so I have to force them into that shape). Next, we normalize our inputs, as gradient descent is highly sensitive to feature magnitudes. This is <strong>super</strong> important. You can check out what happens if you don't normalize your inputs in the ipython notebook. I'll save you some time. It's not pretty. We also commit one of the cardinal sins of machine learning, that is, we train and test on the same dataset. Don't worry we rectify all of these issues in pt2.</p>
</div>
<div class="section" id="output">
<h3>Output</h3>
<div class="highlight"><pre><span class="n">Error</span><span class="p">:</span> <span class="mf">296.146974</span>
<span class="n">Error</span><span class="p">:</span> <span class="mf">264.948349</span>
<span class="n">Error</span><span class="p">:</span> <span class="mf">264.841617</span>
<span class="n">Error</span><span class="p">:</span> <span class="mf">264.819684</span>
<span class="n">Error</span><span class="p">:</span> <span class="mf">264.813792</span>
<span class="n">Error</span><span class="p">:</span> <span class="mf">264.812153</span>
<span class="n">Error</span><span class="p">:</span> <span class="mf">264.811694</span>
<span class="n">Error</span><span class="p">:</span> <span class="mf">264.811566</span>
<span class="n">Error</span><span class="p">:</span> <span class="mf">264.811530</span>
<span class="n">Error</span><span class="p">:</span> <span class="mf">264.811520</span>
<span class="n">Our</span> <span class="n">Error</span><span class="p">:</span> <span class="mf">264.811518</span>
</pre></div>
<p>Well at least we are going in the right direction! We still have some work to do.</p>
</div>
</div>
<div class="section" id="problems">
<h2>Problems</h2>
<ol class="arabic simple">
<li>This linear model has no intercept term $w_0$. How would you add it?</li>
<li>I trained and tested on the same dataset. This is a no no.</li>
</ol>
</div>
<div class="section" id="interesting-things-to-think-about">
<h2>Interesting things to think about</h2>
<ol class="arabic simple">
<li>I set the weights in $W$ to be uniformly randomly sampled about 0. Obviously, in a real linear regression we can solve for the exact weights in $W$. But what happens when we change our error function? If we change our error function to <strong>absolute</strong> difference, is there a closed form solution for $W$? Moreover, if we change our error function to some function $J^{'}$, where there is no closed form solution, is there a way to start the weights at values &quot;close&quot; to some local minimum.</li>
<li>What is the best way to stop our gradient descent? How could we use training/testing sets to make this way more awesome/robust?</li>
</ol>
<p>There you have it. It is all downhill from here, except for the other uphill parts. Feel free to change stuff. It's a free world.</p>
</div>
</div>
    <footer>
<p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">Dan Crescimanno</span>
  </span>
<time datetime="2014-01-13T00:00:00" pubdate>Mon 13 January 2014</time></p><div class="sharing">
</div>    </footer>
  </article>

</div>
<aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="/logistic-classification-code.html">Logistic Classification Code</a>
      </li>
      <li class="post">
          <a href="/building-outward-toward-classification.html">Building outward toward Classification</a>
      </li>
      <li class="post">
          <a href="/linear-regression-code-pt-2.html">Linear Regression Code pt 2</a>
      </li>
      <li class="post">
          <a href="/linear-regression-code-pt-1.html">Linear Regression Code pt 1</a>
      </li>
      <li class="post">
          <a href="/linear-regression.html">Linear Regression</a>
      </li>
    </ul>
  </section>
  <section>
      
    <h1>Categories</h1>
    <ul id="recent_posts">
        <li><a href="/category/python-math.html">python, math</a></li>
    </ul>
  </section>
 

  <section>
  <h1>Tags</h1>
  </section>


    <section>
        <h1>Blogroll</h1>
        <ul>
            <li><a href="http://getpelican.com/" target="_blank">Pelican</a></li>
            <li><a href="http://python.org/" target="_blank">Python.org</a></li>
            <li><a href="http://jinja.pocoo.org/" target="_blank">Jinja2</a></li>
        </ul>
    </section>

</aside><aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="/logistic-classification-code.html">Logistic Classification Code</a>
      </li>
      <li class="post">
          <a href="/building-outward-toward-classification.html">Building outward toward Classification</a>
      </li>
      <li class="post">
          <a href="/linear-regression-code-pt-2.html">Linear Regression Code pt 2</a>
      </li>
      <li class="post">
          <a href="/linear-regression-code-pt-1.html">Linear Regression Code pt 1</a>
      </li>
      <li class="post">
          <a href="/linear-regression.html">Linear Regression</a>
      </li>
    </ul>
  </section>
  <section>
      
    <h1>Categories</h1>
    <ul id="recent_posts">
        <li><a href="/category/python-math.html">python, math</a></li>
    </ul>
  </section>
 

  <section>
  <h1>Tags</h1>
  </section>


    <section>
        <h1>Blogroll</h1>
        <ul>
            <li><a href="http://getpelican.com/" target="_blank">Pelican</a></li>
            <li><a href="http://python.org/" target="_blank">Python.org</a></li>
            <li><a href="http://jinja.pocoo.org/" target="_blank">Jinja2</a></li>
        </ul>
    </section>

</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2013 - Dan Crescimanno -
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
</body>
</html>