<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Building outward toward Classification</title>
  <meta name="author" content="Dan Crescimanno">



  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link href="/favicon.png" rel="icon">
  <link href="/theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">
  <script src="/theme/js/modernizr-2.0.js"></script>
  <script src="/theme/js/ender.js"></script>
  <script src="/theme/js/octopress.js" type="text/javascript"></script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
    });
  </script>

  <script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>


  <link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
</head>

<body>
  <header role="banner"><hgroup>
  <h1><a href="/">What I cannot create, I do not understand</a></h1>
</hgroup></header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/" rel="subscribe-rss">RSS</a></li>
</ul>

<!-- TODO: add search here
<form action="" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
-->

<ul class="main-navigation">
    <!-- TODO: add categories here? -->
</ul></nav>
  <div id="main">
    <div id="content">
<div>
  <article class="hentry" role="article">
<header>
      <h1 class="entry-title">Building outward toward Classification</h1>
      <p class="meta"><time datetime="2014-01-27T00:00:00" pubdate>Mon 27 January 2014</time></p>
</header>

  <div class="entry-content"><div class="section" id="background">
<h2>Background</h2>
<p>So far we have learned about regression, which implies that we are trying to predict a real number. But how does our analysis change if we are trying to predict a class label? A class label is just an integer which represents a certain type of object. Take for example the Iris dataset. It looks like this</p>
<table border="1" class="docutils">
<colgroup>
<col width="17%" />
<col width="15%" />
<col width="17%" />
<col width="15%" />
<col width="8%" />
<col width="14%" />
<col width="13%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head" colspan="4">Inputs</th>
<th class="head" colspan="3">Outputs</th>
</tr>
<tr><th class="head">&nbsp;</th>
<th class="head">&nbsp;</th>
<th class="head">&nbsp;</th>
<th class="head">&nbsp;</th>
<th class="head">&nbsp;</th>
<th class="head">&nbsp;</th>
<th class="head">&nbsp;</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>sepal length</td>
<td>sepal width</td>
<td>petal length</td>
<td>petal width</td>
<td>setosa</td>
<td>versicolor</td>
<td>virginica</td>
</tr>
<tr><td>5.1</td>
<td>3.5</td>
<td>1.4</td>
<td>0.2</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr><td>4.9</td>
<td>3.0</td>
<td>1.4</td>
<td>0.2</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr><td>4.7</td>
<td>3.2</td>
<td>1.3</td>
<td>0.2</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr><td>4.6</td>
<td>3.1</td>
<td>1.5</td>
<td>0.2</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr><td>5.0</td>
<td>3.6</td>
<td>1.4</td>
<td>0.2</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>Let's start with the idea that we are trying to predict a binary class label (0 or 1) for each of the three output flower types.</p>
</div>
<div class="section" id="options">
<h2>Options</h2>
<div class="section" id="good-ol-fashioned-linear-regression">
<h3>Good 'Ol Fashioned Linear Regression</h3>
<p>We could run linear regression just like before with our squared error. Only this time, after our training is completely done, we check the values in $Y$. If a value in $Y$ is greater than 0.5, we map it to 1, and if it is less than 0.5 then we map the result to 0. This stinks for one main reason.</p>
<div class="section" id="problem">
<h4>Problem</h4>
<ol class="arabic simple">
<li>It's hard to rationalize what we are doing. As we are training, we are generating results in $Y$ that are not necessarily between $[0,1]$. This means if our algorithm outputs some $y_{ij}$ that has value $2$, we are going to waste time updating weights in our matrix $W$, which will push $y_{ij}$ closer to 1 (even though we would be classifying it correctly).</li>
</ol>
</div>
<div class="section" id="fix">
<h4>Fix</h4>
<p>This can be fixed if we first map our real numbered output to $[0,1]$, by way of some function. This is also nifty because we can interpret this number as a probability. This provides two major benefits.</p>
</div>
<div class="section" id="benefits">
<h4>Benefits</h4>
<ol class="arabic simple">
<li>If this algorithm is only one part of a machine learning pipeline/bigger software project, then interpreting our output $y_{ij}$ as a probability provides a common interpretation of the output. Now we can chain programs together, as we have some intuition about the outputs of our program.</li>
<li>If management asks what the output of our algorithm represents, saying &quot;probabilities!&quot; sounds much more professional than &quot;uhhh... numbers ($y_{ij}$) which we want to be close to other numbers ($t_{ij}$)&quot;.</li>
</ol>
<p>So let's choose our favorite function that maps the real numbers $R$ onto $[0,1]$. For historical reasons people went with</p>
<p>$$f(x) = Sig(x) = \dfrac{1}{1+e^{-x}}$$</p>
</div>
</div>
<div class="section" id="linear-regression-with-transform">
<h3>Linear Regression with Transform</h3>
<p>After we dot $X \cdot W$, we then map each one of those real values into the range $[0,1]$, by use of our sigmoid function.</p>
<p>$$f(x) = Sig(x) = \dfrac{1}{1+e^{-x}}$$</p>
<p>And so $Y = Sig(X \cdot W)$. From here we can just use our squared error as the error function.</p>
<div class="section" id="id1">
<h4>Problem</h4>
<ol class="arabic simple">
<li>Squared Error doesn't really make sense in the context of probabilities. Consider the following two scenarios. Let's say you feed an example row through your model and output a row of probabilities, $y_{row}$, that represents your predictions for a row of $T$, $t_{row}$. Let's say this row is of length 100, and that $t_{row}$ is the row of all 1's. Namely,</li>
</ol>
<p>$$t_{row} = [1, 1, ..., 1]$$</p>
</div>
<div class="section" id="case-1">
<h4>Case 1:</h4>
<p>Our output $y_{row}$ has the first element 0, and the rest are 1's. It looks like</p>
<p>$$y_{row} = [0, 1, 1, ..., 1]$$</p>
<p>The squared error between $y_{row}$ and $t_{row}$ is then $\dfrac{1}{2}$ (remember we divide squared error by 2).</p>
</div>
<div class="section" id="case-2">
<h4>Case 2:</h4>
<p>Our output $y_{row}$ has .9 for all 100 of its values.</p>
<p>$$y_{row} = [.9, .9, ..., .9]$$</p>
<p>Then our squared error is $\dfrac{100*(.1)^2}{2} = \dfrac{1}{2}$. And so we have equivalent errors.</p>
</div>
<div class="section" id="issue">
<h4>Issue</h4>
<p>Our error function is indifferent to each of these two options. But are we?</p>
<p>Let's say we are predicting the binary values of $t_{row}$ from our $y_{row}$. Let's map everything above .5 to 1, and everything below .5 to 0. We see that in case 1, we only get 99 of our predictions right. Whereas in case 2, we get all 100 binary predictions correct. So it seems we have a preference for case 2. Is there a way to create a better error function that encompasses more of our intuition? Let's do just that.</p>
</div>
</div>
</div>
<div class="section" id="magical-error-function">
<h2>Magical Error Function</h2>
<p>So what do we want our magical error function to do? Well we want it to have the following properties. Before we begin though, let's denote our magical error function as $E$, and remember that it takes in two probabilities, $y_{ij}$ and $t_{ij}$.</p>
<ol class="arabic simple">
<li>We want $E(t_{ij}, t_{ij}) = 0$</li>
<li>We want $E(0, 1)$ to be large (that is as bad as it gets)</li>
<li>We want it to somehow map case 2 to a lower error than case 1.</li>
<li>Bonus points if we can make it cancel some of the added terms that our gradient has accumulated by our addition of the sigmoid to every element.</li>
</ol>
</div>
<div class="section" id="squared-error-recap">
<h2>Squared Error Recap</h2>
<p>Let's revisit our squared error function and gradient, so that we have some context before we go searching for the magical function $E$. I'm going to denote squared error as $SE$. We see that</p>
<p>$$SE(y_{ij}, t_{ij}) = \dfrac{1}{2} (y_{ij} - t_{ij})^2$$
$$\dfrac{dSE}{dy_{ij}} = (y_{ij} - t_{ij})$$</p>
</div>
<div class="section" id="magical-derivative">
<h2>Magical Derivative</h2>
<p>We know that $Y = Sig(X \cdot W)$. To make this even clearer, lets have $Z=X \cdot W$, so that there is no mixup between $Y$ and the dot product of $X \cdot W$. We see that $y_{ij} = Sig(z_{ij})$.</p>
<p>Writing out our expression for the total error, we see that</p>
<p>$$J = \dfrac{1}{m} \sum_{i,j} E(y_{ij}, t_{ij})$$
$$\dfrac{dJ}{dw_{ab}} = \dfrac{1}{m} \sum_{i,j} \dfrac{dE(y_{ij}, t_{ij})}{dy_{ij}}*\dfrac{dy_{ij}}{w_{ab}} = \dfrac{1}{m} \sum_{i,j} \dfrac{dE(y_{ij}, t_{ij})}{y_{ij}} * y_{ij}*(1-y_{ij})*x_{ia}$$</p>
<p>Great! But where did that term $y_{ij}*(1-y_{ij})$ come from? I remember that term $x_{ia}$ from when we took the derivative in linear regression <a class="reference external" href="/linear-regression.html">link</a>.</p>
<p>Let's zoom in on the derivative in question. Remember that $y_{ij} = Sig(z_{ij})$.</p>
<p>$$\dfrac{dy_{ij}}{dw_{ab}} = Sig^{'}(z_{ij}) * \dfrac{dz_{ij}}{dw_{ab}} = Sig^{'}(z_{ij}) * x_{ia}$$</p>
<p>where $j = b$. Remember that term $x_{ia}$ from our <a class="reference external" href="/linear-regression.html">linear regression</a> .
Is there any way that we can simplify this further? Well, we can use the following relationship about $Sig$ to do just that.</p>
<div class="section" id="identities">
<h3>Identities</h3>
<p>$$Sig(x) = \dfrac{1}{1+e^{-x}}$$
$$Sig^{'}(x) = Sig(x)*(1-Sig(x))$$</p>
<p>From our previous expression, we can continue simplifying
$$\dfrac{dy_{ij}}{dw_{ab}} = Sig^{'}(z_{ij}) * x_{ia} = Sig(z_{ij})*(1-Sig(z_{ij}))* x_{ia} = y_{ij}*(1-y_{ij})*x_{ia}$$</p>
</div>
</div>
<div class="section" id="what-is-e">
<h2>What is E?</h2>
<p>So looking back at our original expression we see that</p>
<p>$$\dfrac{dJ}{dw_{ab}} = \dfrac{1}{m} \sum_{i,j} \dfrac{dE(y_{ij}, t_{ij})}{dy_{ij}} * y_{ij}*(1-y_{ij})*x_{ia}$$</p>
<p>Can we make the derivative of $E$ look like the $SE$ gradient (which is  $(y_{ij} - t_{ij})$), and get rid of the pesky addition of $y_{ij}*(1-y_{ij})$?</p>
<p>Namely we want</p>
<p>$$\dfrac{dE(y_{ij}, t_{ij})}{dy_{ij}} = \dfrac{y_{ij} - t_{ij}}{y_{ij}*(1-y_{ij})}$$</p>
<p>Yes, we can! We are a simple integration away! Let's do that integration, and solve for $E$. We get</p>
<p>$$E = \int \dfrac{y_{ij} - t_{ij}}{y_{ij}*(1-y_{ij})} dy_{ij} = -(t_{ij}*log(y_{ij}) + (1-t_{ij})*log(1-y_{ij}))$$</p>
</div>
<div class="section" id="frankenstein-s-monster">
<h2>Frankenstein's Monster</h2>
<p>The thing you have just brought to life is called the &quot;Cross Entropy&quot;. Denote it CE, and it looks like</p>
<p>$$CE(y_{ij}, t_{ij}) = -(t_{ij}*log(y_{ij}) + (1-t_{ij})*log(1-y_{ij}))$$</p>
</div>
<div class="section" id="the-main-derivative">
<h2>The Main Derivative</h2>
<p>By plugging in our error function and allowing a bunch of things to cancel, we can calculate the main derivative.</p>
<p>$$\dfrac{dJ}{dw_{ab}} = \dfrac{1}{m} \sum_{i,j} \dfrac{y_{ij} - t_{ij}}{y_{ij}*(1-y_{ij})} * y_{ij}*(1-y_{ij})*x_{ia} = \dfrac{1}{m} \sum_{i,j} (y_{ij} - t_{ij})x_{ia}$$</p>
<p>Using the same trick as in the linear regression, we can convert this into</p>
<p>$$\dfrac{dJ}{dW} = \dfrac{1}{m}X^{T}(Y-T)$$</p>
<p>but this isn't identical to our linear regression update calculation because $Y$ is <strong>after</strong> the application of Sig.</p>
<p>$$W_{update} = -\alpha \dfrac{dJ}{dW} = -\dfrac{\alpha}{m}X^{T}(Y-T)$$</p>
</div>
<div class="section" id="what-have-we-done">
<h2>What have we done</h2>
<p>Let's recap what we did. Given a dataset where we were trying to predict a binary class label (0 or 1), we created an algorithm that returns us a probability in the range $[0,1]$. This algorithm has 2 major differences from the linear regression that we created last week.</p>
</div>
<div class="section" id="differences">
<h2>Differences</h2>
<ol class="arabic simple">
<li>In normal linear regression, $Y = X \cdot W$.
In our algorithm, $Y = Sig(X \cdot W)$.</li>
<li>Instead of using the squared error as our error function, we went with the cross-entropy error, which looks like</li>
</ol>
<p>$$CE(y_{ij}, t_{ij}) = -(t_{ij}*log(y_{ij}) + (1-t_{ij})*log(1-y_{ij}))$$</p>
<p>We did this because we are comparing probabilities.</p>
<p>The model still looks like this</p>
<img alt="Logistic Classification" src="theme/images/LogisticClassification.png" />
</div>
<div class="section" id="creation">
<h2>Creation</h2>
<p>This thing that you have created is called &quot;logistic regression&quot;. But the reality is, it doesn't matter what it is called, because you now understand how it was created. After that entire derivation, it should be obvious to you that this name is <em>terrible</em>. This algorithm performs classification, not regression.</p>
<blockquote>
For the remainder of this blog, and for the remainder of my life, I'm going to call this thing &quot;Logistic Classification&quot;. Feel free to join me.</blockquote>
</div>
<div class="section" id="interesting-ideas">
<h2>Interesting Ideas</h2>
<p>When I write the expression $\dfrac{A}{B}$ for matrices $A$, and $B$, I am talking about the <strong>element-wise</strong> division of those matrices. If I wanted to multiply $A$, by the inverse of $B$, I would write $A \cdot B^{-1}$. Moreover, when I want to multiply two matrices together <strong>element-wise</strong>, I write either $AB$ or $A * B$. If I wanted to <strong>dot</strong> two matrices together I write it like $A \cdot B$.</p>
<p>In our post on linear regression, we created this rule called the <strong>Partial North</strong>. Does this rule work for logistic classification? We were able to write</p>
<p>$$\dfrac{dJ}{dW} = X^T \cdot \dfrac{dJ}{dY}$$</p>
<p>But what is $\dfrac{dJ}{dY}$ for Cross-Entropy Error? We know that</p>
<p>$$\dfrac{dCE(y_{ij}, t_{ij})}{dy_{ij}} = \dfrac{y_{ij} - t_{ij}}{y_{ij}(1-y_{ij})}$$</p>
<p>So every element of $\dfrac{dJ}{dY}$, must be that, so then</p>
<p>$$\dfrac{dJ}{dY} = \dfrac{1}{m} \dfrac{Y-T}{Y(1-Y)}$$</p>
<p>Hmm... But then it looks like our rule doesn't work because we would calculate</p>
<p>$$\dfrac{dJ}{dW} = X^T \cdot \dfrac{1}{m} \dfrac{Y-T}{Y(1-Y)}$$</p>
<p>And this is not what we got above. So what did we miss?</p>
</div>
<div class="section" id="chain-rule">
<h2>Chain Rule</h2>
<p>If you look at the analysis above for the derivative of $J$, we realize that we are not computing the chain rule through the derivative of our Sig function.
Take for example the sum in linear regression. It looks vaguely like this</p>
<div class="section" id="linear-regression-sum">
<h3>Linear Regression Sum</h3>
<dl class="docutils">
<dt>$$\sum_{i,j} (y_{ij} - t_{ij})x_{ia} =</dt>
<dd>\sum_{i,j} \dfrac{dSE(y_{ij}, t_{ij}}{dy_{ij}x_{ia}$$</dd>
</dl>
<p>Eventually that $x_{ia}$ turns into our matrix $X^{T}$, and the derivative turns into $\dfrac{dJ}{dY}$.</p>
<p>In contrast the sum in logistic classification looks vaguely like</p>
</div>
<div class="section" id="logistic-classification-sum">
<h3>Logistic Classification Sum</h3>
<p>$$\sum_{i,j} \dfrac{CE(y_{ij}, t_{ij})}{y_{ij}}y_{ij}*(1-y_{ij})x_{ia}$$</p>
<p>Well, it looks like that term $y_{ij}(1-y_{ij})$ is really the difference. We remember that this term is really $Sig^{'}(z_{ij})$. So what we realize is that we need to take the derivative of our $f^{'}$ and multiply it <strong>element-wise</strong> to each element.</p>
<p>If we think of this in matrix notation, we get the following.</p>
<p>$$\dfrac{dJ}{dY} * f^{'}(X \cdot W)$$</p>
<p>Note that $X \cdot W$ will generate the linear output of our model, which is $Z$ in our analysis above. Then we use the derivative of our function, and multiply that element-wise to $\dfrac{dJ}{dY}$.</p>
<p>Using what we have learned from this analysis, you can write the <strong>Generalized Partial North</strong>. It looks like</p>
</div>
</div>
<div class="section" id="generalized-partial-north">
<h2>Generalized Partial North</h2>
<p>If we have $Y = f(X \cdot W)$, then we know</p>
<p>$$\dfrac{dJ}{dW} = X^{T} \cdot \left(\dfrac{dJ}{dY} * f^{'}(X \cdot W) \right)$$</p>
<p>I'll show you how to use this rule, along with it's counterpart as we head into Neural Net territory.</p>
</div>
<div class="section" id="future">
<h2>Future</h2>
<p>You derived a way above to compute the matrix of partials which are <strong>north</strong> of your current matrix of partials. Moreover, you can now use any function that you want (not just Sig), and it still works!</p>
<p>So you have figured out how to move your partials north in a generalized way, but how do you move them <strong>west</strong>, and why is that important? Don't worry we will derive this next rule soon, as we move into the twisted land of Neural Nets. What is a neural net, you ask? Neural nets are just generalizations of these models but with more internal matrices.
A neural net with 2 internal matrices looks like this.</p>
<img alt="3-Layer Neural Net" src="theme/images/3-LayerNeuralNetv2.png" />
<p>We'll get there.</p>
</div>
<div class="section" id="code">
<h2>Code</h2>
<p>The time is nigh for you to take a crack at this function. I'm going to borrow most of the stuff from the linear regression with minor tweaks. Once we get this right, we are going to kick this machine learning stuff into high gear. Hope you are ready.</p>
<blockquote>
We are going to build this as an extension to our linear regression.</blockquote>
<p>Here is the outline of our LogisticClassification.</p>
<div class="highlight"><pre><span class="k">class</span> <span class="nc">LogisticClassification</span><span class="p">:</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">error_func</span><span class="o">=</span><span class="n">CrossEntropyError</span><span class="p">,</span>
               <span class="n">matrix_func</span><span class="o">=</span> <span class="n">Sig</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
               <span class="n">W</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>
    <span class="o">...</span>

  <span class="k">def</span> <span class="nf">error</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
    <span class="o">...</span>

  <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="o">...</span>

  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
    <span class="o">...</span>

  <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span><span class="n">T</span><span class="p">):</span>
    <span class="o">...</span>
</pre></div>
<p>I'm going to test our Logistic on the famous Iris dataset. It is easy to grab as it ships with the sklearn.datasets. See you space cowboy.</p>
</div>
</div>
    <footer>
<p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">Dan Crescimanno</span>
  </span>
<time datetime="2014-01-27T00:00:00" pubdate>Mon 27 January 2014</time></p><div class="sharing">
</div>    </footer>
  </article>

</div>
<aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="/logistic-classification-code.html">Logistic Classification Code</a>
      </li>
      <li class="post">
          <a href="/building-outward-toward-classification.html">Building outward toward Classification</a>
      </li>
      <li class="post">
          <a href="/linear-regression-code-pt-2.html">Linear Regression Code pt 2</a>
      </li>
      <li class="post">
          <a href="/linear-regression-code-pt-1.html">Linear Regression Code pt 1</a>
      </li>
      <li class="post">
          <a href="/linear-regression.html">Linear Regression</a>
      </li>
    </ul>
  </section>
  <section>
      
    <h1>Categories</h1>
    <ul id="recent_posts">
        <li><a href="/category/python-math.html">python, math</a></li>
    </ul>
  </section>
 

  <section>
  <h1>Tags</h1>
  </section>


    <section>
        <h1>Blogroll</h1>
        <ul>
            <li><a href="http://getpelican.com/" target="_blank">Pelican</a></li>
            <li><a href="http://python.org/" target="_blank">Python.org</a></li>
            <li><a href="http://jinja.pocoo.org/" target="_blank">Jinja2</a></li>
        </ul>
    </section>

</aside><aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="/logistic-classification-code.html">Logistic Classification Code</a>
      </li>
      <li class="post">
          <a href="/building-outward-toward-classification.html">Building outward toward Classification</a>
      </li>
      <li class="post">
          <a href="/linear-regression-code-pt-2.html">Linear Regression Code pt 2</a>
      </li>
      <li class="post">
          <a href="/linear-regression-code-pt-1.html">Linear Regression Code pt 1</a>
      </li>
      <li class="post">
          <a href="/linear-regression.html">Linear Regression</a>
      </li>
    </ul>
  </section>
  <section>
      
    <h1>Categories</h1>
    <ul id="recent_posts">
        <li><a href="/category/python-math.html">python, math</a></li>
    </ul>
  </section>
 

  <section>
  <h1>Tags</h1>
  </section>


    <section>
        <h1>Blogroll</h1>
        <ul>
            <li><a href="http://getpelican.com/" target="_blank">Pelican</a></li>
            <li><a href="http://python.org/" target="_blank">Python.org</a></li>
            <li><a href="http://jinja.pocoo.org/" target="_blank">Jinja2</a></li>
        </ul>
    </section>

</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2013 - Dan Crescimanno -
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
</body>
</html>