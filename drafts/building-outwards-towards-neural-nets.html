<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Building outwards towards Neural Nets</title>
  <meta name="author" content="Dan">



  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link href="/favicon.png" rel="icon">
  <link href="/theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">
  <script src="/theme/js/modernizr-2.0.js"></script>
  <script src="/theme/js/ender.js"></script>
  <script src="/theme/js/octopress.js" type="text/javascript"></script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
    });
  </script>

  <script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>


  <link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
</head>

<body>
  <header role="banner"><hgroup>
  <h1><a href="/">Test</a></h1>
</hgroup></header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/" rel="subscribe-rss">RSS</a></li>
</ul>

<!-- TODO: add search here
<form action="" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
-->

<ul class="main-navigation">
    <!-- TODO: add categories here? -->
</ul></nav>
  <div id="main">
    <div id="content">
<div>
  <article class="hentry" role="article">
<header>
      <h1 class="entry-title">Building outwards towards Neural Nets</h1>
      <p class="meta"><time datetime="2014-03-25T00:00:00" pubdate>Tue 25 March 2014</time></p>
</header>

  <div class="entry-content"><p>This post assumes you have read the posts on <a class="reference external" href="/linear-regression.html">linear regression</a>, and <a class="reference external" href="/building-outward-toward-classification.html">logistic classification</a>.</p>
<div class="section" id="neural-nets">
<h2>Neural Nets</h2>
<p>Today we are going to build the framework for neural nets. Well what are neural nets? Here is a picture of one</p>
<img alt="2-Layer Neural Net" src="theme/images/2-layer-neural.png" style="height: 400px;" />
<p>These are not the only type of neural nets. You can connect nodes to themselves, and have edges that go &quot;backwards&quot;. But for right now, I'm restricting my attention to neural nets where every layer is fully connected to the layer right after it. Here is another simpler neural net, that is only 1 layer deep.</p>
<img alt="1-Layer Neural Net" src="theme/images/1-layer-neural.png" style="height: 400px;" />
<p>Ok, but what does this simple diagram represent? When people talk about neural nets they usually have some input values which they assign to the nodes on the very left. Then these values are passed along each edge to the nodes on the right. In transit though, these values are multiplied by a unique edge value (present in all edges). Afterwards. all of the inputs coming into a node on the right are summed up. Then there is an optional function that is applied to the values in the nodes on the right.</p>
</div>
<div class="section" id="matrices">
<h2>Matrices</h2>
<p>If you are anything like me, and you just read the previous paragraph, then you are a white hot ball of rage right now. The reason why you are gnashing your teeth and breaking things, is because you realize this is a colossally verbose and stupidly complex way of multiplying your input by a matrix (and then optionally applying a function). Here is that same picture again, but with the weights in the edges organized in matrix form.</p>
<img alt="Layer as Matrix" src="theme/images/layer-matrix.png" style="height: 400px;" />
<p>We see that the input row is $x_{row}$, and the output is just $x_{row} \cdot W$. Normally in a real neural net, one would apply a function to each element in that resulting row, turning it into $f(x_{row} \cdot W$. In words, we see that one layer of a neural net is a matrix and a function.</p>
<p>I want you to pause and really ponder the differences between the last two images. The relationship between these two drawings is the relationship between &quot;The Cloud&quot; and &quot;A Server Somewhere&quot;. Technically, they are the same thing, but one <strong>sounds</strong> way sexier. You can write papers about &quot;The Cloud&quot;. Very rarely are papers written about &quot;A Server Somewhere&quot;. Those papers don't get read, and they don't get grants. So on an intellectual level you can understand maybe how one image became temporarily more popular, but it is impossible to stop that small voice in the back of your head from shouting</p>
<blockquote>
Please for the love of all that is green on this Earth, use matrices. It's simpler.</blockquote>
</div>
<div class="section" id="networks">
<h2>Networks</h2>
<p>After creating such a creature, people began stringing them together to make more complicated networks like this. This is simply another layer added after the first layer, to make a larger network.</p>
<img alt="2-Layer Neural Net" src="theme/images/2-layer-neural.png" style="height: 400px;" />
<p>Correspondingly, my image devoid of riff-raff is</p>
<img alt="2-Layer Matrix View" src="theme/images/2-layer-matrix.png" style="height: 500px;" />
<p>If you are not used to seeing matrix multiplication drawn this way, please read the post on <a class="reference external" href="/linear-regression.html">linear regression</a>.</p>
<p>But that is just one input. We can imagine passing in a bunch of inputs at once to create something like this.</p>
<img alt="Larger Models" src="theme/images/LargerModels.png" />
<p>So we see pictorially that the model we are creating for neural nets is just a generalization of the models that we have created for linear regression and logistic classification. We just have more internal matrices now (and a wider variety of functions to apply after the dot). Well, I'll be darned!</p>
<blockquote>
Like every gradient descent problem, the goal is to update the internal matrices to make Y close to T.</blockquote>
<p>So the next question becomes, how do we update our internal matrices? We can do what we have always done which is just compute the partial derivatives of our error function with respect to every weight, and just walk down in error. That would definitely work. But what about this new-fangled approach called &quot;backpropagation&quot; that sounds so sexy and is all abuzz right now. Well I'm about to let the cat out of the bag</p>
<blockquote>
Backpropagation IS the computation of the partial derivatives</blockquote>
</div>
<div class="section" id="backpropagation">
<h2>Backpropagation</h2>
<p>That's right folks! You heard it here first. They are one in the same.  This brings me to one of the things that really bothers me about neural nets/backpropagation, namely that they are shrouded in mysticism. Every time I look up backpropagation, I get a series of arcane steps that are equivalent to medieval alchemy. Conventional descriptions of backpropagation usually begin with an image like this one, linked <a class="reference external" href="http://www.youtube.com/watch?v=wmfpS5fKFeY">here</a> (it's on youtube).</p>
<img alt="Backprop" src="theme/images/backprop_ng2.png" style="height: 400px;" />
<p>The problems here begin with a wildly new notation to represent every node. Then there are these &quot;rules&quot; to compute the &quot;error&quot; of previous nodes. It's unclear where these rules came from. It's also unclear what exactly these &quot;errors&quot; represent (in my derivation, you realize that they are partial derivatives). My favorite part is at moment 4:30, where magically some equations leap onto the screen. Awesome! What I love most in the world are equations with no context!</p>
<p>Other images of backpropagation include the likes of this</p>
<img alt="Jesus" src="theme/images/BackPropCalculations.png" style="height: 400px;" />
<p>The problem is not that the teachers sidestep the math, quite the opposite. The problem is that the math is just sprayed out there with little justification. The math just <em>is</em>. This is just how backprop <em>works</em>.</p>
<p>Look, I get it. I understand that teachers are quick to write the mystery math, so that we (the reader) can write this mystery code and move on with our day. But <strong>ideas are only understood as an extension to our own thinking</strong>. Telling someone an answer is not knowledge. Telling someone a story, wherein they can create an answer, is knowledge.</p>
<blockquote>
The goal of the teacher is to weave a narrative so compelling that you write the end of the story.</blockquote>
<p>I poke fun at these renditions of backpropagation. But even though they are complex, they do contain a wealth of solid information, if one puts in the time. That much can't be said for &quot;backpropagation&quot; as explained by ...</p>
</div>
<div class="section" id="wikipedia">
<h2>Wikipedia</h2>
<p>God have mercy on your soul if you ever try to read the pseudocode for <a class="reference external" href="http://en.wikipedia.org/wiki/Backpropagation">backpropagation</a> on wikipedia. Nothing could be more of an abstraction. I'm surprised they didn't go with something like</p>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">backpropagate</span><span class="p">():</span>
  <span class="n">compute</span> <span class="n">delta</span> <span class="k">for</span> <span class="nb">all</span> <span class="n">weights</span>
  <span class="n">update</span> <span class="n">weights</span>
</pre></div>
<p>I mean, give me a break. How is any newcomer going to learn a damn thing from that &quot;pseudocode&quot;. Even looking at the real code is about as clear as mud. This snippet is taken from the python code linked at the bottom of wikipedia.</p>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">backPropagate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">):</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">no</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;wrong number of target values&#39;</span><span class="p">)</span>

  <span class="c"># calculate error terms for output</span>
  <span class="n">output_deltas</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">no</span>
  <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">no</span><span class="p">):</span>
      <span class="n">error</span> <span class="o">=</span> <span class="n">targets</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">ao</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
      <span class="n">output_deltas</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">dsigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ao</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="o">*</span> <span class="n">error</span>

  <span class="c"># calculate error terms for hidden</span>
  <span class="n">hidden_deltas</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">nh</span>
  <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nh</span><span class="p">):</span>
      <span class="n">error</span> <span class="o">=</span> <span class="mf">0.0</span>
      <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">no</span><span class="p">):</span>
          <span class="n">error</span> <span class="o">=</span> <span class="n">error</span> <span class="o">+</span> <span class="n">output_deltas</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">wo</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>
      <span class="n">hidden_deltas</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">dsigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ah</span><span class="p">[</span><span class="n">j</span><span class="p">])</span> <span class="o">*</span> <span class="n">error</span>

  <span class="c"># update output weights</span>
  <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nh</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">no</span><span class="p">):</span>
          <span class="n">change</span> <span class="o">=</span> <span class="n">output_deltas</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">ah</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">wo</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wo</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">N</span><span class="o">*</span><span class="n">change</span> <span class="o">+</span> <span class="n">M</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">co</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">co</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">change</span>
          <span class="c">#print N*change, M*self.co[j][k]</span>

  <span class="c"># update input weights</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ni</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nh</span><span class="p">):</span>
          <span class="n">change</span> <span class="o">=</span> <span class="n">hidden_deltas</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">ai</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">wi</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wi</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">N</span><span class="o">*</span><span class="n">change</span> <span class="o">+</span> <span class="n">M</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">ci</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">ci</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">change</span>

  <span class="c"># calculate error</span>
  <span class="n">error</span> <span class="o">=</span> <span class="mf">0.0</span>
  <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">targets</span><span class="p">)):</span>
      <span class="n">error</span> <span class="o">=</span> <span class="n">error</span> <span class="o">+</span> <span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="n">targets</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">ao</span><span class="p">[</span><span class="n">k</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span>
  <span class="k">return</span> <span class="n">error</span>
</pre></div>
<p>Now what in the hell is going on here? Lots of things are getting calculated, but there is obviously a breakdown in <strong>meaning</strong>. What are we computing? And why are we computing it? And is this the best way to do that? This code is warning code about the dangers of not using matrices. I mean just look at it. The author has decided to do matrix operations <strong>without using matrix operations</strong>.</p>
</div>
<div class="section" id="the-beginning">
<h2>The beginning</h2>
<blockquote>
The hard way is easier -- Zed Shaw</blockquote>
<p>Sit down. Take out your pencil and take the partial derivative. Let's derive backpropagation. Someone else did it, and so can you.</p>
</div>
<div class="section" id="derivative">
<h2>Derivative</h2>
<p>With two rules you are going to rederive the &quot;backpropagation&quot; algorithm, also known as the &quot;compute partial derivatives&quot; algorithm. You already know one of them. This first rule I call <strong>Partial North</strong>, and you derived it way back in the post on <a class="reference external" href="/linear-regression.html">linear regression</a>.</p>
<p>Basically, you had a model like this.</p>
<img alt="Linear Regression" src="theme/images/LinReg.png" style="height: 400px;" />
<p>From this image, you were able to compute $\dfrac{dJ}{dY}$. This quantity is a matrix of size $Y$, where the element at index $(i,j)$ is $\dfrac{dJ}{dy_{ij}}$. I like to think of this matrix as the partials <strong>in their correct place</strong>. If you are unfamiliar with this notation please read the <a class="reference external" href="/linear-regression.html">linear regression</a> post.</p>
<p>From that, you figured out that you could compute $\dfrac{dJ}{dW}$, which is exactly the quantity you needed in order to update your weights.</p>
</div>
<div class="section" id="partial-north">
<h2>Partial North</h2>
<p>$$\dfrac{dJ}{dW} = X^{T} \cdot \dfrac{dJ}{dY}$$</p>
<p>But how does that help us with the larger models? Let's assume for the time being that there are no functions applied after the dot.</p>
<img alt="Larger Models" src="theme/images/LargerModels.png" />
<p>This means that $Y = X_0 \cdot W_0$, and that $X_0 = X_1 \cdot W_1$.</p>
<p>There are no assumptions in the derivation of this rule, where I specified where exactly in the larger model I am. I mean to say that if I had the partials $\dfrac{dJ}{dY}$, then I could calculate</p>
<p>$$\dfrac{dJ}{dW_0} = X_0^{T} \cdot \dfrac{dJ}{dY}$$</p>
<p>Moreover, if I had the partials $\dfrac{dJ}{dX_0}$, I could calculate the other matrix partial of interest, namely</p>
<p>$$\dfrac{dJ}{dW_1} = X_1^{T} \cdot \dfrac{dJ}{dX_0}$$</p>
<p>And so you see the game now becomes finding an expression for $\dfrac{dJ}{dX_0}$, in terms of $X_0$, $W_0$, and $\dfrac{dJ}{dY}$. In this way, we can begin to walk our partials <strong>west</strong>. If the other rule is called <strong>Partial North</strong>, I'm sure you can guess what this rule is called.</p>
</div>
<div class="section" id="quick-aside">
<h2>Quick Aside</h2>
<p>This version of the  <strong>Partial North</strong> implies that you are not applying a function after you dot $X$ and $W$ (ie.. linear regression). We are going to derive the <strong>Partial West</strong> rule below for the special case we don't apply a function after the dot.</p>
<p>For more interesting models (like logistic classification and neural nets), you absolutely do apply functions after the dot ($X \cdot W$ say). And in doing so, you have to update your <strong>Partial North</strong> and <strong>Partial West</strong> to account for that. Don't worry, we do the more general case down below.</p>
<p>How exactly does one create the ...</p>
</div>
<div class="section" id="partial-west">
<h2>Partial West</h2>
<p>So let's compute out $\dfrac{dJ}{X_0}$, and see if we can simplify it. Before we begin, I want to rename $X_0$ to $X$, and $W_0$ to $W$, just so that I don't have to keep track of those indices during this calculation. Feel free to put them back after.</p>
<p>So we are trying to calculate $\dfrac{dJ}{dX}$.</p>
</div>
<div class="section" id="id5">
<h2>Derivative</h2>
<p>What do we know?</p>
<p>$$J = \dfrac{1}{2m} \sum_{i,j} (y_{ij} - t_{ij})^{2} $$</p>
<p>$$\dfrac{dJ}{dx_{ab}} = \dfrac{1}{m} \sum_{i,j} \dfrac{dJ}{dy_{ij}} * \dfrac{dy_{ij}}{dx_{ab}} = \dfrac{1}{m} \sum_{i,j} (y_{ij} - t_{ij})\dfrac{dy_{ij}}{dx_{ab}}$$</p>
<p>To get a sense of that second term consider the following pictures. This analysis is very similar to that done with linear regression.</p>
<img alt="Partial in x" src="theme/images/partial_y_x.png" />
<p>Given that $X \cdot W = Y$, consider a random element $y_{ij}$, and some equally random element of the $X$ matrix $x_{ab}$. How is $y_{ij}$ changed if we vary $x_{ab}$? It's pretty easy to see that changing $x_{ab}$ has no effect on $y_{ij}$, if $i \neq a$, because they are not in the same row.</p>
<img alt="Partial in x" src="theme/images/partial_y_x_row.png" />
<p>If they are in the same row, it is not hard to see that</p>
<p>$$\dfrac{dy_{ij}}{dx_{ab}} = w_{bj}$$
if $a = i$, otherwise it is 0.</p>
<p>So the expression becomes</p>
<p>$$\dfrac{dJ}{dx_{ab}} = \dfrac{1}{m} \sum_{j} (y_{aj} - t_{aj})w_{bj}$$</p>
<p>Because we can set $i = a$. Hmmm... So how can we simplify this expression? Let's consider one element, say $x_{23}$, and imagine what we are summing up to compute that value. Well we can stare at this <strong>awesome</strong> <a class="reference external" href="http://codepen.io/dcrmls/pen/JjgrI">visualization</a> of that calculation for value $x_{23}$. And if you glare at this for a while, take a swig of strong cognac, and tilt our head to the side, it might become clear to you how to proceed.</p>
<p>What we really want is to dot one row in $\dfrac{dJ}{dY}$ with another row in $W$. So if we take that one row in $\dfrac{dJ}{dY}$ and dot it with the <strong>transpose</strong> of that row in $W$, we get exactly what we want.</p>
<p>$$\dfrac{dJ}{dx_{ab}} = \dfrac{1}{m} (Y_{a\_} - T_{a\_}) \cdot W_{b\_}^{T}$$</p>
<p>Collecting those terms into a big 'ol matrix we find that</p>
<p>$$\dfrac{dJ}{dX} = \dfrac{1}{m} (Y-T) \cdot W^{T}$$</p>
<p>or more concisely</p>
<p>$$\dfrac{dJ}{dX} = \dfrac{dJ}{dY} \cdot W^{T}$$</p>
<p>This is what I call the <strong>Partial West</strong></p>
</div>
<div class="section" id="id6">
<h2>Partial West</h2>
<p>$$\dfrac{dJ}{dX} = \dfrac{dJ}{dY} \cdot W^{T}$$</p>
<p>Woah! Pretty sweet! Again, in this derivation, I made no assumptions about where I was in the larger model. But these two rules both assume that we are not applying a function after we dot $X$ and $W$. How can we make this generalize?</p>
<p>We derive the generalization of the <strong>Partial North</strong> rule in the post on <a class="reference external" href="/building-outward-toward-classification.html">logistic classification</a>. I suggest you read it if you haven't because we can use the same ideas to generalize the <strong>Partial West</strong>.</p>
<p>Let's now assume that $Z = X \cdot W$, and $Y = f(Z)$. So we now have a matrix $Y$ which is <strong>post</strong> transform.</p>
<p>Basically in the logistic classification post we realized that we need to apply the chain rule when we computed our derivative. But what element is multiplied to the $(i,j)$ element of $\dfrac{dJ}{dY}$? We need to multiply it by $f^{'}(z_{ij})$. This term falls right out of the chain rule when we compute the derivative.</p>
<p>In matrix form you can write out the generalized <strong>West Partial</strong>.</p>
<p>$$\dfrac{dJ}{dX} = \left(\dfrac{dJ}{dY} * f^{'}(X \cdot W)\right) \cdot W^{T}$$</p>
<p>If this seems like a leap, check out the derivation of the logistic classification algorithm, and look at what we multiply our $\dfrac{dJ}{dY}$ by. The link is <a class="reference external" href="/building-outward-toward-classification.html">right here</a></p>
</div>
<div class="section" id="generalization">
<h2>Generalization</h2>
<p>Here we are assuming $Y = f(X \cdot W)$</p>
<div class="section" id="generalized-north-partial">
<h3>Generalized North Partial</h3>
<p>$$\dfrac{dJ}{dW} = X^{T} \cdot \left(\dfrac{dJ}{dY} * f^{'}(X \cdot W)\right)$$</p>
</div>
<div class="section" id="generalized-west-partial">
<h3>Generalized West Partial</h3>
<p>$$\dfrac{dJ}{dX} = \left(\dfrac{dJ}{dY} * f^{'}(X \cdot W)\right) \cdot W^{T}$$</p>
<p>Note that $\cdot$ is the dot product, and $*$ is an element-wise multiplication. Big difference.</p>
<p>Just for future reference, when I write the expression $\dfrac{A}{B}$ for matrices $A$, and $B$, I am talking about the <strong>element-wise</strong> division of those matrices. If I wanted to multiply $A$, by the inverse of $B$, I would write $A \cdot B^{-1}$. Moreover, when I want to multiply two matrices together <strong>element-wise</strong>, I write either $AB$ or $A * B$. If I wanted to <strong>dot</strong> two matrices together I write it like $A \cdot B$.</p>
</div>
</div>
<div class="section" id="nice">
<h2>Nice!</h2>
<p>And there you have it. The keys to the hovercar. From here, we see that we can recursively use these two rules to take steps west and north. This will create the partial derivative of whatever internal matrix, $W$, that we need. So armed with our two rules, we can now calculate just about anything for this class of generalized models.</p>
<p>Now it might not be obvious just how awesome these rules are just yet. So to make that clear and show how to use them, let's recreate all of the math for all of the models that we have build up to this point.</p>
</div>
<div class="section" id="linear-regression-with-squared-error">
<h2>Linear Regression with Squared Error</h2>
<p>We know that
$$\dfrac{dJ}{dY} = \dfrac{1}{m} (Y-T)$$</p>
<p>$$\dfrac{dJ}{dW} = X^{T} \cdot \dfrac{dJ}{dY} = \dfrac{1}{m} X^{T} \cdot (Y - T)$$</p>
</div>
<div class="section" id="logistic-classification-with-cross-entropy-error">
<h2>Logistic Classification with Cross-Entropy Error</h2>
<p>What is $\dfrac{dJ}{dY}$ for cross entropy error?</p>
<p>In the logistic regression post, we derived that</p>
<p>$$\dfrac{dCE(y_{ij}, t_{ij})}{dy_{ij}} = \dfrac{y_{ij} - t_{ij}}{y_{ij}(1-y_{ij})}$$</p>
<p>So every element of $\dfrac{dJ}{dY}$, must be that, so then</p>
<p>$$\dfrac{dJ}{dY} = \dfrac{1}{m} \dfrac{Y-T}{Y(1-Y)}$$</p>
<p>Element-wise division above.</p>
<p>So then we see that</p>
<p>$$\dfrac{dJ}{dW} = X^{T} \cdot \left(\dfrac{dJ}{dY} * Sig^{'}(X \cdot W) \right) = X^{T} \cdot \left( \dfrac{1}{m} \dfrac{Y-T}{Y(1-Y)} * Y(1-Y) \right) = \dfrac{1}{m} X^T \cdot (Y-T)$$</p>
<p>Well that was certainly faster.</p>
<div class="section" id="trick">
<h3>Trick</h3>
<p>I used the trick above that we derived in the logistical classification post, namely that</p>
<p>$$Sig^{'}(X \cdot W) = Sig(X\cdot W)(1-Sig(X\cdot W)) = Y(1-Y)$$</p>
<p>I use this trick below as well.</p>
</div>
</div>
<div class="section" id="thoughts">
<h2>Thoughts</h2>
<p>The main benefit of all of this stuff is that the math suddenly becomes plug-and-playable. If you wanted to consider a totally different error function, then all you need to do is create the corresponding $\dfrac{dJ}{dY}$, and the rest holds. If you wanted to try a different function $f$ after one of your dots ($X$ and $W$), then you just need to compute $f^{'}(X \cdot W)$, and everything else holds.</p>
</div>
<div class="section" id="neural-net-2-layers-cross-entropy-error">
<h2>Neural Net 2 Layers Cross-Entropy Error</h2>
<p>Let's say that this Neural net has a sigmoid function attached to both of it's layers. $f = Sig$ in the picture below.</p>
<img alt="3 Layer Neural Net" src="theme/images/3-LayerNeuralNetv2.png" />
<p>Let's start walking our error back.</p>
<div class="section" id="partials-end">
<h3>Partials End</h3>
<p>$$\dfrac{dJ}{dY} = \dfrac{Y - T}{Y(1-Y)}$$</p>
</div>
<div class="section" id="id8">
<h3>Partial North</h3>
<p>$$\dfrac{dJ}{dW_0} = X_0^{T} \cdot \left(\dfrac{dJ}{dY} * Sig^{'}(X_0 \cdot W_0) \right) = \dfrac{1}{m} X_0^{T} \cdot (Y - T)$$</p>
</div>
<div class="section" id="id9">
<h3>Partial West</h3>
<p>$$\dfrac{dJ}{dX_0} = \left(\dfrac{dJ}{dY} * Sig^{'}(X_0 \cdot W_0) \right) \cdot W_0^{T} = \dfrac{1}{m} (Y - T) \cdot W_0^{T}$$</p>
</div>
<div class="section" id="id10">
<h3>Partial North</h3>
<p>$$\dfrac{dJ}{dW_1} = X_1^{T} \cdot \left(\dfrac{dJ}{dX_0} * Sig^{'}(X_1 \cdot W_1) \right)
= \dfrac{1}{m} X_1^{T} \cdot \left((Y-T) \cdot W_0^{T} * X_0(1-X_0)\right)$$</p>
<p>Yes, that last expression is kinda big and ugly. Now, I want you to imagine doing that calculation <strong>without matrices</strong>. While you may never do this by hand again, it is nice to know that you are basically doing calculus through matrices, and that the rules for propagating partials are ones that you have created. Once you code this up, the alternation of the north and west partials will calculate the update to your internal matrices.</p>
</div>
</div>
<div class="section" id="time-to-code">
<h2>Time to Code</h2>
<p>Now this is the fun part. It's time to write the Neural Net. I'll see you next week.</p>
<p>For me:</p>
<ol class="arabic simple">
<li>change image to sig</li>
</ol>
</div>
</div>
    <footer>
<p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">Dan Crescimanno</span>
  </span>
<time datetime="2014-03-25T00:00:00" pubdate>Tue 25 March 2014</time></p><div class="sharing">
</div>    </footer>
  </article>

</div>
<aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="/logistic-classification-code.html">Logistic Classification Code</a>
      </li>
      <li class="post">
          <a href="/building-outward-toward-classification.html">Building outward toward Classification</a>
      </li>
      <li class="post">
          <a href="/linear-regression-code-pt-2.html">Linear Regression Code pt 2</a>
      </li>
      <li class="post">
          <a href="/linear-regression-code-pt-1.html">Linear Regression Code pt 1</a>
      </li>
      <li class="post">
          <a href="/linear-regression.html">Linear Regression</a>
      </li>
    </ul>
  </section>
  <section>
      
    <h1>Categories</h1>
    <ul id="recent_posts">
        <li><a href="/category/python-math.html">python, math</a></li>
    </ul>
  </section>
 

  <section>
  <h1>Tags</h1>
  </section>


    <section>
        <h1>Social</h1>
        <ul>
            <li><a href="/" type="application/rss+xml" rel="alternate">RSS</a></li>
            <li><a href="#" target="_blank">You can add links in your config file</a></li>
            <li><a href="#" target="_blank">Another social link</a></li>
        </ul>
    </section>
    <section>
        <h1>Blogroll</h1>
        <ul>
            <li><a href="http://getpelican.com/" target="_blank">Pelican</a></li>
            <li><a href="http://python.org/" target="_blank">Python.org</a></li>
            <li><a href="http://jinja.pocoo.org/" target="_blank">Jinja2</a></li>
            <li><a href="#" target="_blank">You can modify those links in your config file</a></li>
        </ul>
    </section>

</aside><aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="/logistic-classification-code.html">Logistic Classification Code</a>
      </li>
      <li class="post">
          <a href="/building-outward-toward-classification.html">Building outward toward Classification</a>
      </li>
      <li class="post">
          <a href="/linear-regression-code-pt-2.html">Linear Regression Code pt 2</a>
      </li>
      <li class="post">
          <a href="/linear-regression-code-pt-1.html">Linear Regression Code pt 1</a>
      </li>
      <li class="post">
          <a href="/linear-regression.html">Linear Regression</a>
      </li>
    </ul>
  </section>
  <section>
      
    <h1>Categories</h1>
    <ul id="recent_posts">
        <li><a href="/category/python-math.html">python, math</a></li>
    </ul>
  </section>
 

  <section>
  <h1>Tags</h1>
  </section>


    <section>
        <h1>Social</h1>
        <ul>
            <li><a href="/" type="application/rss+xml" rel="alternate">RSS</a></li>
            <li><a href="#" target="_blank">You can add links in your config file</a></li>
            <li><a href="#" target="_blank">Another social link</a></li>
        </ul>
    </section>
    <section>
        <h1>Blogroll</h1>
        <ul>
            <li><a href="http://getpelican.com/" target="_blank">Pelican</a></li>
            <li><a href="http://python.org/" target="_blank">Python.org</a></li>
            <li><a href="http://jinja.pocoo.org/" target="_blank">Jinja2</a></li>
            <li><a href="#" target="_blank">You can modify those links in your config file</a></li>
        </ul>
    </section>

</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2013 - Dan -
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
</body>
</html>