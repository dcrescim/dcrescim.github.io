<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Matrices and Derivatives</title>
  <meta name="author" content="Dan">



  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link href="/favicon.png" rel="icon">
  <link href="/theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">
  <script src="/theme/js/modernizr-2.0.js"></script>
  <script src="/theme/js/ender.js"></script>
  <script src="/theme/js/octopress.js" type="text/javascript"></script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
    });
  </script>

  <script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>


  <link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
</head>

<body>
  <header role="banner"><hgroup>
  <h1><a href="/">Test</a></h1>
</hgroup></header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/" rel="subscribe-rss">RSS</a></li>
</ul>

<!-- TODO: add search here
<form action="" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
-->

<ul class="main-navigation">
    <!-- TODO: add categories here? -->
</ul></nav>
  <div id="main">
    <div id="content">
<div>
  <article class="hentry" role="article">
<header>
      <h1 class="entry-title">Matrices and Derivatives</h1>
      <p class="meta"><time datetime="2014-03-17T00:00:00" pubdate>Mon 17 March 2014</time></p>
</header>

  <div class="entry-content"><div class="section" id="ideas">
<h2>Ideas</h2>
<p>I promised an easier way of calculating partial derivatives through matrices. Let's define some new notation.</p>
<p>Let</p>
<p>$$\dfrac{dJ}{dW}$$</p>
<p>represent the matrix of derivatives partial derivatives. It looks like this</p>
<img alt="Matrix_Deriv" src="theme/images/PartialDerivMatrices.png" />
<p>It is a matrix of size $W$, and each element is the partial derivative of J with respect to that element. It is the matrix that we are going to multiply by -1 and add to our previous W, to update it.</p>
<p>This quantity is what every algorithm is trying to compute. It's the matrix of partial derivatives of the error.</p>
<p>Let this function $P$, represent the act of &quot;propogating&quot; error from one value/matrix to another matrix. For example</p>
<p>$$P(J, Y)$$</p>
<p>would be a matrix of size $Y$ which has in element $y_{ij}$ the derivative $\dfrac{dJ}{y_{ij}}$.</p>
<p>Given that what would
Also, let's assume that $Y = X*W$</p>
<p>$$P(Y,W)$$</p>
<p>represent? Well, it would represent a matrix of size $W$, where the element of $w_{ab}$ would house the partial derivative of $\sum_{i,j}\dfrac{dy_{ij}}{dw_{ab}} * y_error_{ij}$.</p>
<p>Let's pick $w_{11}$, and let's make a matrix $Y_e$ is the matrix of errors stored in a matrix of size $Y$.</p>
<p>Here is an image of what is going on</p>
<p>[image]</p>
<p>All of the elements that $w{11}$ influences are illustrated in light orange. The derivative of each of those elements with respect to $w_{11}$ are illustrated in light green. Now we have to dot these things together because we are computing the partial derivative, which means all of the errors that we have calculated, have to be multiplied by the derivatives at that point.</p>
<p>That looks like this
[image take two vectors out and dot them]</p>
<p>If you stare at this for a while, drink some fine tequila, and cock your head to the side, it turns into this, and we can simplify the algebra for this operation. And we see that</p>
<p>[image]</p>
<p>$$P(Y,W) = X^{T}*Y$$</p>
<p>What have we just done? Well we have shown that if you have your error stored in a matrix the size of $Y$, then the partial derivatives for $W$ have the form $X^{T}*Y$</p>
<p>Let's say we have the error in the matrix $Y$, and we are trying to find
$$P(Y,X)$$</p>
<p>Here is the image, and we find by lemma Tequila that</p>
<p>[image]</p>
<p>$$P(Y,X) = Y*W^T$$</p>
<p>What have you just done? Well you have written down just how partial derivatives travel through matrices. And now using the two rules you have just seen, you can derive 40% of machine learning. Let's see this in action!</p>
<p>Ok, this is where the arcane stops. If you have gotten this far, I'm impressed. &quot;Ok Dan, but what is this all good for?&quot;, you might be thinking. If you buy all this, then all of the algebra becomes easy. I mean to say that getting backpropogation (read matrix of partial derivatives).</p>
<p>Linear Regression:</p>
<img alt="Linear Regression" src="theme/images/LinRegv2.png" />
<p>Y = X*W, F=Identity, J = squared error</p>
<p>$$dfrac{dJ}{W} = P(P(J, Y),W) = P(dfrac{1}{m}(T-Y), W) = X^{T}*dfrac{1}{m}(T-Y)</p>
<p>Logistic Regression:</p>
<img alt="Linear Regression" src="theme/images/LogRegv2.png" />
<p>Y = X*W, F_W = Sig, J = Classification Error</p>
<p>$$\dfrac{dJ}{W} = P(P(J,Y), W) = P(\dfrac{1}{m}(T-Y)./(Y .* (1-Y)) .* \dfrac{dF}{dY}, W) = P(\dfrac{1}{m}(T-Y), W) = \dfrac{1}{m}X^T*(T-Y)</p>
<p>The perceptron is special case of logistic regression</p>
<p>Neural Net (2 matrices deep):</p>
<img alt="2-Matrix Neural Net" src="theme/images/3-LayerNeuralNetv2.png" />
<p>Y = X_0*W_0, F_{W0}, X_0 = X_1*W_1, J = Squared Error</p>
<p>$$\dfrac{dJ}{W_0} = P(P(J,Y), W) = P(\dfrac{1}{m}(T-Y) .* \dfrac{dF}{dY}, W)
= P(\dfrac{1}{m}(T-Y), W)
= \dfrac{1}{m}X^T*((T-Y).*F^{'}(X_0*W_0))$$</p>
<p>$$\dfrac{dJ}{W_1} = P(P(P(J,Y), X_1), W_1)$$</p>
<p>Starting from the inside out, we have</p>
<p>$$first = P(J,Y) = \dfrac{1}{m}(T-Y)$$</p>
<p>$$second = P(first, X_1) = \dfrac{1}{m}(T-Y)*F^{'}(X_0*W_0)*W^{T}$$</p>
<p>$$third = P(second, W_1) = X_1^{T}*(\dfrac{1}{m}(T-Y)*F^{'}(X_0*W_0)*W^{T})$$</p>
<p>We see a recursive pattern that is being generated here. Looking at our first two matrices, it is not hard to guess the general case. It looks like this</p>
<p>Neural Net (N matrices deep):
J = squared error.
dfrac{dJ}{W_0} = X_0^T(dfrac{1}{m}*(T-Y).*F^{'}(X_0*W_0))
dfrac{dJ}{W_1} = X_1^T(dfrac{1}{m}*(T-Y).*F_{W_0}^{'}(X_0*W_0)*W_0^{T}.*F_{W_1}^{'}(X_1*W_1))</p>
<p>So we see some sort of A-B recursion, we create some variables to make this palatable</p>
<p>Let
$$mat_delt(0) = dfrac{1}{m}(T-Y) or more generally P(J,Y)$$
$$mat_delt(n) = mat_delt(n-1).*F_{W_{n-1}}^{'}(X_{n-1}*W_{n-1}) * W_{n-1}^T $$</p>
<p>Then
$$dfrac{dJ}{W_n} = X_n^{T}*mat_delt(n)$$</p>
</div>
<div class="section" id="code">
<h2>Code</h2>
<p>Here is my code for backpropogation</p>
<div class="highlight"><pre>  delta_cur = self.grad(output, truth)*lr
  for layer in rev_layers:
    delta_cur = delta_cur * layer.deriv(layer.linear_output)
    W_update = np.dot(layer.input.T,delta_cur)
    delta_cur = np.dot(delta_cur,layer.W.T)
    layer.W -= W_update
</pre></div>
</div>
<div class="section" id="goodbye-liebnitz">
<h2>Goodbye, Liebnitz</h2>
<p>We can't celebrate with champagne just yet. Matrices are not commutative</p>
<p>$$X*Y neq Y*X$$</p>
<p>which means that the higher level analogue of the chain rule through matrices is not commutative. So, let's say we wanted to break up this like we would in calc</p>
<p>$$dfrac{dJ}{dW} = dfrac{dJ}{Y} * dfrac{Y}{W}$$</p>
</div>
</div>
    <footer>
<p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">Dan Crescimanno</span>
  </span>
<time datetime="2014-03-17T00:00:00" pubdate>Mon 17 March 2014</time></p><div class="sharing">
</div>    </footer>
  </article>

</div>
<aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="/building-outwards-towards-neural-nets.html">Building outwards towards Neural Nets</a>
      </li>
      <li class="post">
          <a href="/logistic-classification-code.html">Logistic Classification Code</a>
      </li>
      <li class="post">
          <a href="/building-outward-toward-classification.html">Building outward toward Classification</a>
      </li>
      <li class="post">
          <a href="/linear-regression-code-pt-2.html">Linear Regression Code pt 2</a>
      </li>
      <li class="post">
          <a href="/linear-regression-code-pt-1.html">Linear Regression Code pt 1</a>
      </li>
    </ul>
  </section>
  <section>
      
    <h1>Categories</h1>
    <ul id="recent_posts">
        <li><a href="/category/python-math.html">python, math</a></li>
    </ul>
  </section>
 

  <section>
  <h1>Tags</h1>
  </section>


    <section>
        <h1>Social</h1>
        <ul>
            <li><a href="/" type="application/rss+xml" rel="alternate">RSS</a></li>
            <li><a href="#" target="_blank">You can add links in your config file</a></li>
            <li><a href="#" target="_blank">Another social link</a></li>
        </ul>
    </section>
    <section>
        <h1>Blogroll</h1>
        <ul>
            <li><a href="http://getpelican.com/" target="_blank">Pelican</a></li>
            <li><a href="http://python.org/" target="_blank">Python.org</a></li>
            <li><a href="http://jinja.pocoo.org/" target="_blank">Jinja2</a></li>
            <li><a href="#" target="_blank">You can modify those links in your config file</a></li>
        </ul>
    </section>

</aside><aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="/building-outwards-towards-neural-nets.html">Building outwards towards Neural Nets</a>
      </li>
      <li class="post">
          <a href="/logistic-classification-code.html">Logistic Classification Code</a>
      </li>
      <li class="post">
          <a href="/building-outward-toward-classification.html">Building outward toward Classification</a>
      </li>
      <li class="post">
          <a href="/linear-regression-code-pt-2.html">Linear Regression Code pt 2</a>
      </li>
      <li class="post">
          <a href="/linear-regression-code-pt-1.html">Linear Regression Code pt 1</a>
      </li>
    </ul>
  </section>
  <section>
      
    <h1>Categories</h1>
    <ul id="recent_posts">
        <li><a href="/category/python-math.html">python, math</a></li>
    </ul>
  </section>
 

  <section>
  <h1>Tags</h1>
  </section>


    <section>
        <h1>Social</h1>
        <ul>
            <li><a href="/" type="application/rss+xml" rel="alternate">RSS</a></li>
            <li><a href="#" target="_blank">You can add links in your config file</a></li>
            <li><a href="#" target="_blank">Another social link</a></li>
        </ul>
    </section>
    <section>
        <h1>Blogroll</h1>
        <ul>
            <li><a href="http://getpelican.com/" target="_blank">Pelican</a></li>
            <li><a href="http://python.org/" target="_blank">Python.org</a></li>
            <li><a href="http://jinja.pocoo.org/" target="_blank">Jinja2</a></li>
            <li><a href="#" target="_blank">You can modify those links in your config file</a></li>
        </ul>
    </section>

</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2013 - Dan -
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
</body>
</html>