<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Matrices and Derivatives Rewrite</title>
  <meta name="author" content="Dan">



  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link href="/favicon.png" rel="icon">
  <link href="/theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">
  <script src="/theme/js/modernizr-2.0.js"></script>
  <script src="/theme/js/ender.js"></script>
  <script src="/theme/js/octopress.js" type="text/javascript"></script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
    });
  </script>

  <script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>


  <link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
</head>

<body>
  <header role="banner"><hgroup>
  <h1><a href="/">Test</a></h1>
</hgroup></header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/" rel="subscribe-rss">RSS</a></li>
</ul>

<!-- TODO: add search here
<form action="" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
-->

<ul class="main-navigation">
    <!-- TODO: add categories here? -->
</ul></nav>
  <div id="main">
    <div id="content">
<div>
  <article class="hentry" role="article">
<header>
      <h1 class="entry-title">Matrices and Derivatives Rewrite</h1>
      <p class="meta"><time datetime="2014-03-17T00:00:00" pubdate>Mon 17 March 2014</time></p>
</header>

  <div class="entry-content"><div class="section" id="setup">
<h2>Setup</h2>
<p>Let's take a 10,000 ft view of everything we just learned. And then let's anticipate where we think everything is going.</p>
<p>This is what linear regression looks like
[image]</p>
<p>This is what logistic classification looks like
[image]</p>
<p>What do they have in common. Well, for one, we see that the there are some operations that we should attach to the $W$ matrix. It can have a function application afterword (like in the case of Sig for logistic classification). The partial derivatives are calculated almost identically in these two scenarios.</p>
<p>Let's write a &quot;layer&quot; class that generalizes this concept.</p>
</div>
<div class="section" id="layer">
<h2>Layer</h2>
<div class="highlight"><pre><span class="k">class</span> <span class="nc">Layer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">matrix_func</span><span class="o">=</span><span class="n">Identity</span><span class="p">):</span>

  <span class="k">if</span> <span class="n">W</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">a</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">dim</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span>
      <span class="n">low</span> <span class="o">=</span> <span class="o">-</span><span class="n">a</span><span class="p">,</span>
      <span class="n">high</span> <span class="o">=</span> <span class="n">a</span><span class="p">,</span>
      <span class="n">size</span> <span class="o">=</span> <span class="n">dim</span>
    <span class="p">))</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">b</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">a</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">dim</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span>
      <span class="n">low</span> <span class="o">=</span> <span class="o">-</span><span class="n">a</span><span class="p">,</span>
      <span class="n">high</span> <span class="o">=</span> <span class="n">a</span><span class="p">,</span>
      <span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="p">))</span>

  <span class="k">else</span><span class="p">:</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

  <span class="bp">self</span><span class="o">.</span><span class="n">matrix_func</span> <span class="o">=</span> <span class="n">matrix_func</span>
</pre></div>
<p>This is a generalized version of what we have above. The &quot;matrix_func&quot; represents the function that we apply to each element of our matrix after we have dotted it with $W$, and added the intercept.</p>
<p>So, a linear layer of size (2,2), and a logistic layer would be created like this, respectively</p>
<div class="highlight"><pre><span class="n">linear_layer</span> <span class="o">=</span> <span class="n">Layer</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">matrix_func</span><span class="o">=</span><span class="n">Identity</span><span class="p">)</span>
<span class="n">logistic_layer</span> <span class="o">=</span> <span class="n">Layer</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">matrix_func</span><span class="o">=</span><span class="n">Sig</span><span class="p">)</span>
</pre></div>
<p>We also know that the propogation of our partial derivatives is very similar in these cases too</p>
</div>
<div class="section" id="partial-north">
<h2>Partial North</h2>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">backward_north</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">partials</span><span class="p">):</span>
  <span class="n">partials</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matrix_func</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_output</span><span class="p">)</span>
  <span class="n">W_partial</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">partials</span><span class="p">)</span>
  <span class="n">b_partial</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">partials</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="k">return</span> <span class="p">(</span><span class="n">W_partial</span><span class="p">,</span> <span class="n">b_partial</span><span class="p">)</span>
</pre></div>
<p>It's easy to see that we need the gradient of our matrix_func, for this backward step. Remember for example, our Logistic Classification. In it, we need to multiply by $y_{ij}*(1-y_{ij})$. That is the derivative of our Sig function applied to our linear output.</p>
<p>So this makes us aware of three things.</p>
<ol class="arabic simple">
<li>Basically, every error function, and matrix func needs to be stored with both the original function for the <em>forward</em> pass, and the derivative needed for the <em>backward</em> pass of partials.</li>
<li>We need to save the linear output, on the <em>forward</em> pass, because we need to multiply the partials by our chain ruled matrix_func derivative.</li>
<li>We need to save the input <em>X</em>, because we need it to calculate $\dfrac{dJ}{dW} = X^{T} \cdot \dfrac{dJ}{dY}$, on the <em>backward</em> pass.</li>
</ol>
</div>
<div class="section" id="forward">
<h2>Forward</h2>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
  <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span>
  <span class="bp">self</span><span class="o">.</span><span class="n">linear_output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>
  <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">func</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_output</span><span class="p">)</span>
</pre></div>
<p>Moreover whenever we have a model, that has more than one layer (or internal matrix/func combo), we need to be able to propogate the partials back to them.</p>
<p>In the first post I alluded to the fact that we need to calculate $\dfrac{dJ}{dX}$. We need this value so that we can then use our <em>partial north</em> to compute the matrix of partials for our other internal matrices. Consider the following picture with two internal matrices.</p>
<p>[image with matrices]</p>
</div>
<div class="section" id="derivative">
<h2>Derivative</h2>
<p>Let's pretend for a moment, that we had a model where $X$ was filled with variables that we could change, and $W$ was filled with things that we couldn't.</p>
<p>Let's compute</p>
<p>$$\dfrac{dJ}{dx_{ab}} = stuff $$</p>
<p>which implies that
$$\dfrac{dJ}{dX} = \dfrac{dJ}{dY} \cdot W^{T}$$</p>
<p>Fix that.</p>
<p>I like to call that the <strong>partial west</strong> (because we are moving the partials west in our model). Moreover, I like to call the other calculation the <strong>partial north</strong> because it moves a partial up one matrix in our model. That calculation looks like</p>
<p>$$\dfrac{dJ}{dW} = X^{T} \cdot \dfrac{dJ}{dY}$$</p>
<p>It's pretty amazing that from these two things, you can make models as arbitrarily large as you want. As you can see, the pushing of the partials recurses on itself as you move it westward.</p>
<p>Here I add one more function to our layer</p>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">backward_west</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">partials</span><span class="p">):</span>
  <span class="n">partials</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matrix_func</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_output</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">partials</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>
<p>If these pictures don't make sense, take a look at the beginning of this blog</p>
<p>[linear regression link]</p>
<p>We see that every model has this same sort of structure. Namely</p>
</div>
</div>
    <footer>
<p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">Dan Crescimanno</span>
  </span>
<time datetime="2014-03-17T00:00:00" pubdate>Mon 17 March 2014</time></p><div class="sharing">
</div>    </footer>
  </article>

</div>
<aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="/building-outwards-towards-neural-nets.html">Building outwards towards Neural Nets</a>
      </li>
      <li class="post">
          <a href="/logistic-classification-code.html">Logistic Classification Code</a>
      </li>
      <li class="post">
          <a href="/building-outward-toward-classification.html">Building outward toward Classification</a>
      </li>
      <li class="post">
          <a href="/linear-regression-code-pt-2.html">Linear Regression Code pt 2</a>
      </li>
      <li class="post">
          <a href="/linear-regression-code-pt-1.html">Linear Regression Code pt 1</a>
      </li>
    </ul>
  </section>
  <section>
      
    <h1>Categories</h1>
    <ul id="recent_posts">
        <li><a href="/category/python-math.html">python, math</a></li>
    </ul>
  </section>
 

  <section>
  <h1>Tags</h1>
  </section>


    <section>
        <h1>Social</h1>
        <ul>
            <li><a href="/" type="application/rss+xml" rel="alternate">RSS</a></li>
            <li><a href="#" target="_blank">You can add links in your config file</a></li>
            <li><a href="#" target="_blank">Another social link</a></li>
        </ul>
    </section>
    <section>
        <h1>Blogroll</h1>
        <ul>
            <li><a href="http://getpelican.com/" target="_blank">Pelican</a></li>
            <li><a href="http://python.org/" target="_blank">Python.org</a></li>
            <li><a href="http://jinja.pocoo.org/" target="_blank">Jinja2</a></li>
            <li><a href="#" target="_blank">You can modify those links in your config file</a></li>
        </ul>
    </section>

</aside><aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="/building-outwards-towards-neural-nets.html">Building outwards towards Neural Nets</a>
      </li>
      <li class="post">
          <a href="/logistic-classification-code.html">Logistic Classification Code</a>
      </li>
      <li class="post">
          <a href="/building-outward-toward-classification.html">Building outward toward Classification</a>
      </li>
      <li class="post">
          <a href="/linear-regression-code-pt-2.html">Linear Regression Code pt 2</a>
      </li>
      <li class="post">
          <a href="/linear-regression-code-pt-1.html">Linear Regression Code pt 1</a>
      </li>
    </ul>
  </section>
  <section>
      
    <h1>Categories</h1>
    <ul id="recent_posts">
        <li><a href="/category/python-math.html">python, math</a></li>
    </ul>
  </section>
 

  <section>
  <h1>Tags</h1>
  </section>


    <section>
        <h1>Social</h1>
        <ul>
            <li><a href="/" type="application/rss+xml" rel="alternate">RSS</a></li>
            <li><a href="#" target="_blank">You can add links in your config file</a></li>
            <li><a href="#" target="_blank">Another social link</a></li>
        </ul>
    </section>
    <section>
        <h1>Blogroll</h1>
        <ul>
            <li><a href="http://getpelican.com/" target="_blank">Pelican</a></li>
            <li><a href="http://python.org/" target="_blank">Python.org</a></li>
            <li><a href="http://jinja.pocoo.org/" target="_blank">Jinja2</a></li>
            <li><a href="#" target="_blank">You can modify those links in your config file</a></li>
        </ul>
    </section>

</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2013 - Dan -
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
</body>
</html>