<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Matrices and Derivatives Rewrite</title>
  <meta name="author" content="Dan">



  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link href="/favicon.png" rel="icon">
  <link href="/theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">
  <script src="/theme/js/modernizr-2.0.js"></script>
  <script src="/theme/js/ender.js"></script>
  <script src="/theme/js/octopress.js" type="text/javascript"></script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
    });
  </script>

  <script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>


  <link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
</head>

<body>
  <header role="banner"><hgroup>
  <h1><a href="/">Test</a></h1>
</hgroup></header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/" rel="subscribe-rss">RSS</a></li>
</ul>

<!-- TODO: add search here
<form action="" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
-->

<ul class="main-navigation">
    <!-- TODO: add categories here? -->
</ul></nav>
  <div id="main">
    <div id="content">
<div>
  <article class="hentry" role="article">
<header>
      <h1 class="entry-title">Matrices and Derivatives Rewrite</h1>
      <p class="meta"><time datetime="2014-03-17T00:00:00" pubdate>Mon 17 March 2014</time></p>
</header>

  <div class="entry-content"><p>Last time we did this [link].</p>
<div class="section" id="setup">
<h2>Setup</h2>
<p>Let's take a 10,000 ft view of everything we just learned. We should look for the similar pieces in linear regression, logistic classification, and neural nets.</p>
</div>
<div class="section" id="things-we-have-learned">
<h2>Things we have learned</h2>
<ol class="arabic simple">
<li>The concept of a &quot;layer&quot;, which is just a matrix and a function, is at the core of all the models we have build thus far. Normally we have some input to a layer $X$, and we dot it with the layer's internal function $W$. Then we apply our internal function to that remaining quantity ($f(X \cdot W$), and pass that on. Let's refer to this $f$ as the &quot;matrix_func&quot;.</li>
<li>Every error function, and matrix_func needs to be stored with it's derivative, because that is needed for the <em>backward</em> pass of partials. The function itself is need for the <em>forward</em> pass.</li>
<li>Instead of recomputing $X \cdot W$ again on the <em>backward</em> (needed for $f^{'}(X \cdot W)$) pass of our partials, this quantity can be stored on the forward pass (saving us time), and then reused.</li>
</ol>
<p>4. We need to save the input $X$, because we need it for our calculation of the <strong>North Partial</strong>. We already save the matrix $W$, so we got the <strong>West Partial</strong> covered.
Layer
-----</p>
<p>Let's flesh out this idea more. This is what a layer should feel like</p>
<div class="highlight"><pre><span class="k">class</span> <span class="nc">Layer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">matrix_func</span><span class="o">=</span><span class="n">Identity</span><span class="p">):</span>
    <span class="o">...</span>

  <span class="c"># Feed X through my layer (and generate output)</span>
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="o">...</span>

  <span class="c"># given partials, compute the partials for our weight matrix</span>
  <span class="k">def</span> <span class="nf">north_partial</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">partials</span><span class="p">):</span>
    <span class="o">...</span>

  <span class="c"># Given partials, compute the partials west of me</span>
  <span class="k">def</span> <span class="nf">west_partial</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">partials</span><span class="p">):</span>
    <span class="o">...</span>

  <span class="c"># Some toString method</span>
  <span class="k">def</span> <span class="nf">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="o">...</span>
</pre></div>
<p>Now the &quot;matrix_func&quot; represents the function that we apply to each element of our matrix after we have dotted it with $W$, and added our intercept, $b$.</p>
<p>If we have this as our layer, we can create a linear layer (used in a linear regression), or a sigmoid layer (used in logistic classification) as follows</p>
<div class="highlight"><pre><span class="n">linear_layer</span> <span class="o">=</span> <span class="n">Layer</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span><span class="n">matrix_func</span><span class="o">=</span><span class="n">Identity</span><span class="p">)</span>
<span class="n">logistic_layer</span> <span class="o">=</span> <span class="n">Layer</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">matrix_func</span><span class="o">=</span><span class="n">Sig</span><span class="p">)</span>
</pre></div>
<p>Let's take these functions one by one</p>
</div>
<div class="section" id="constructor">
<h2>Constructor</h2>
<div class="highlight"><pre>  if W is None:
    a = 1. / dim[0]
    self.W = np.array(np.random.uniform(
      low = -a,
      high = a,
      size = dim
    ))
  else:
    self.W = np.copy(W)

  if b is None:
    a = 1. / dim[0]
    self.b = np.array(np.random.uniform(
      low = -a,
      high = a,
      size = (1, dim[1])
    ))

  else:
    self.b = np.copy(b)

  self.matrix_func = matrix_func
</pre></div>
<p>This just sets up our matrix $W$, and $b$, and saves our matrix_func accordingly. To reiteratie, Nothing super special here.</p>
</div>
<div class="section" id="forward">
<h2>Forward</h2>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
  <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span>
  <span class="bp">self</span><span class="o">.</span><span class="n">linear_output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>
  <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">matrix_func</span><span class="o">.</span><span class="n">func</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_output</span><span class="p">)</span>
</pre></div>
<p>We remember to store the linear output of this layer, so that we can use that later on the backward pass.</p>
</div>
<div class="section" id="north-partial">
<h2>North Partial</h2>
<p>These both just follow from the equations that we derived in the last post. We use the trick used here [link] to create the partials for our intercept $b$.</p>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">north_partial</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">partials</span><span class="p">):</span>
  <span class="n">W_partial</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">partials</span><span class="p">)</span>
  <span class="n">b_partial</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">partials</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="k">return</span> <span class="p">(</span><span class="n">W_partial</span><span class="p">,</span> <span class="n">b_partial</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="west-partial">
<h2>West Partial</h2>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">west_partial</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">partials</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">partials</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>
<p>This was derived last post.</p>
<p>Now it is time to make the Neural Network which is just the combination of layers that we have created above.</p>
</div>
<div class="section" id="neural-network">
<h2>Neural Network</h2>
<p>To me a Neural Network is just a collection
Now, obviously the first question is what should we pass to this thing. Now obviously we could spell out the dimensions of every layer inside but that would be verbose (especially considering the fact that they &quot;connect&quot; together).</p>
<p>So what exactly do we need? Well we really only need to specify the x dimension of the matrix in the layer. This is because the y dimension is completely determined by the previous layer's $x$ dimension. Moreover, we can separate the out the dimension from the input. In my mind a &quot;layer&quot; has an an internal matrix and function. It stands to reason then that the size of the input into the whole function is not a layer. Let's consider the following picture.</p>
<p>[picture</p>
<p>labelled input dimension
]</p>
<p>So let's say we wanted a neural net to do linear regression on the Boston housing data set. The instantiation would look like this.</p>
<div class="highlight"><pre><span class="n">N</span> <span class="o">=</span> <span class="n">NN</span><span class="p">(</span><span class="n">input_dim</span> <span class="o">=</span> <span class="mi">13</span><span class="p">,</span> <span class="n">layer_list</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="n">Identity</span><span class="p">)],</span> <span class="n">error_func</span><span class="o">=</span><span class="n">SquaredError</span><span class="p">)</span>
</pre></div>
<p>If we wanted to create a logistic classification (say for the Iris dataset), then we could write</p>
<div class="highlight"><pre><span class="n">N</span> <span class="o">=</span> <span class="n">NN</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">layer_list</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">3</span><span class="p">,</span> <span class="n">Sig</span><span class="p">)],</span> <span class="n">error_func</span> <span class="o">=</span> <span class="n">CrossEntropyError</span><span class="p">)</span>
</pre></div>
<p>But let's say we wanted to add another internal matrix to the mix. Then we could do.</p>
<div class="highlight"><pre><span class="n">N</span> <span class="o">=</span> <span class="n">NN</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">layer_list</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">2</span><span class="p">,</span> <span class="n">Identity</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">Sig</span><span class="p">)],</span> <span class="n">error_func</span> <span class="o">=</span> <span class="n">CrossEntropyError</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="neural-net">
<h2>Neural Net</h2>
<p>We are going to implement the standard scikit-learn function calls for this Estimator</p>
<div class="highlight"><pre><span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">layer_list</span> <span class="o">=</span> <span class="p">[],</span> <span class="nb">type</span><span class="o">=</span><span class="s">&#39;R&#39;</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
  <span class="o">...</span>

<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
  <span class="o">...</span>

<span class="k">def</span> <span class="nf">error</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
  <span class="o">...</span>

<span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
  <span class="o">...</span>

<span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
  <span class="o">...</span>
</pre></div>
<p>Let's take easiest to hardest</p>
</div>
<div class="section" id="predict">
<h2>Predict</h2>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
  <span class="n">current_results</span> <span class="o">=</span> <span class="n">X</span>
  <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
    <span class="n">current_results</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">current_results</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">current_results</span>
</pre></div>
<p>We just call the forward function for each layer.</p>
</div>
<div class="section" id="error">
<h2>Error</h2>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">error</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
  <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
  <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">error_func</span><span class="o">.</span><span class="n">func</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
</pre></div>
<p>Exact same pattern as before. Get the output, and then call your error function on it.</p>
</div>
<div class="section" id="id1">
<h2>Constructor</h2>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">layer_list</span> <span class="o">=</span> <span class="p">[],</span> <span class="nb">type</span><span class="o">=</span><span class="s">&#39;R&#39;</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
  <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
  <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>
  <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span> <span class="o">=</span> <span class="n">n_iter</span>

  <span class="c"># Classification problem</span>
  <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s">&#39;C&#39;</span><span class="p">:</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">error_func</span> <span class="o">=</span> <span class="n">CrossEntropyError</span>
  <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s">&#39;R&#39;</span><span class="p">:</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">error_func</span> <span class="o">=</span> <span class="n">SquaredError</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">assert</span> <span class="s">&quot;Unsupported type. &#39;C&#39; (for classification) or &#39;R&#39; for regression&quot;</span>

  <span class="k">if</span> <span class="n">layer_list</span><span class="p">:</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">init_layer_list</span><span class="p">(</span><span class="n">layer_list</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">init_layer_list</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer_list</span><span class="p">):</span>
  <span class="c"># Split up the dimensions and matrix_funcs</span>
  <span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="n">matrix_funcs</span><span class="p">)</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">layer_list</span><span class="p">)</span>
  <span class="n">dims</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">dims</span><span class="p">)</span>
  <span class="n">dims</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>

  <span class="c"># Make matrix dimensions</span>
  <span class="n">dims</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">dims</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>

  <span class="c"># Make matrix_funcs</span>
  <span class="n">matrix_funcs</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">mapping</span><span class="p">[</span><span class="n">x</span><span class="p">],</span> <span class="n">matrix_funcs</span><span class="p">)</span>

  <span class="c"># Zip &#39;em back together</span>
  <span class="n">args</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="n">matrix_funcs</span><span class="p">)</span>

  <span class="k">for</span> <span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">matrix_func</span><span class="p">)</span> <span class="ow">in</span> <span class="n">args</span><span class="p">:</span>
    <span class="n">layer</span> <span class="o">=</span> <span class="n">Layer</span><span class="p">(</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span><span class="p">,</span> <span class="n">matrix_func</span> <span class="o">=</span> <span class="n">matrix_func</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>
</pre></div>
<p>And here we talk about the decisions we made above (fix)</p>
<p>And last but finally not least, we write our backprop function (unassumingly called update)</p>
</div>
<div class="section" id="update">
<h2>Update</h2>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
  <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
  <span class="n">cur_partial</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">error_func</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">T</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">lr</span>
  <span class="n">rev_layers</span> <span class="o">=</span> <span class="nb">reversed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">rev_layers</span><span class="p">:</span>

    <span class="c"># Chain rule through our matrix_func</span>
    <span class="n">cur_partial</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matrix_func</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_output</span><span class="p">)</span>

    <span class="c">#Compute the partial north, and west</span>
    <span class="p">(</span><span class="n">W_partial</span><span class="p">,</span> <span class="n">b_partial</span><span class="p">)</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">north_partial</span><span class="p">(</span><span class="n">cur_partial</span><span class="p">)</span>
    <span class="n">cur_partial</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">west_partial</span><span class="p">(</span><span class="n">cur_partial</span><span class="p">)</span>

    <span class="c"># Update matrix and intercept</span>
    <span class="n">layer</span><span class="o">.</span><span class="n">W</span> <span class="o">-=</span> <span class="n">W_partial</span>
    <span class="n">layer</span><span class="o">.</span><span class="n">b</span> <span class="o">-=</span> <span class="n">b_partial</span>
</pre></div>
<p>Woah! Very nice and perdy. This also works for a generic $n$ levels. You can contrast that with the backprop from last post (which only works for 2 internal matrices).</p>
</div>
<div class="section" id="fit">
<h2>Fit</h2>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>

  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span><span class="p">):</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
      <span class="k">print</span> <span class="s">&quot;Error: </span><span class="si">%f</span><span class="s">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">T</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">T</span><span class="p">)</span>
</pre></div>
<p>We also know that the propagation of our partial derivatives is very similar in these cases too</p>
<p>It's easy to see that we need the gradient of our matrix_func, for this backward step. Remember for example, our Logistic Classification. In it, we need to multiply by $y_{ij}*(1-y_{ij})$. That is the derivative of our Sig function applied to our linear output.</p>
</div>
</div>
    <footer>
<p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">Dan Crescimanno</span>
  </span>
<time datetime="2014-03-17T00:00:00" pubdate>Mon 17 March 2014</time></p><div class="sharing">
</div>    </footer>
  </article>

</div>
<aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="/logistic-classification-code.html">Logistic Classification Code</a>
      </li>
      <li class="post">
          <a href="/building-outward-toward-classification.html">Building outward toward Classification</a>
      </li>
      <li class="post">
          <a href="/linear-regression-code-pt-2.html">Linear Regression Code pt 2</a>
      </li>
      <li class="post">
          <a href="/linear-regression-code-pt-1.html">Linear Regression Code pt 1</a>
      </li>
      <li class="post">
          <a href="/linear-regression.html">Linear Regression</a>
      </li>
    </ul>
  </section>
  <section>
      
    <h1>Categories</h1>
    <ul id="recent_posts">
        <li><a href="/category/python-math.html">python, math</a></li>
    </ul>
  </section>
 

  <section>
  <h1>Tags</h1>
  </section>


    <section>
        <h1>Social</h1>
        <ul>
            <li><a href="/" type="application/rss+xml" rel="alternate">RSS</a></li>
            <li><a href="#" target="_blank">You can add links in your config file</a></li>
            <li><a href="#" target="_blank">Another social link</a></li>
        </ul>
    </section>
    <section>
        <h1>Blogroll</h1>
        <ul>
            <li><a href="http://getpelican.com/" target="_blank">Pelican</a></li>
            <li><a href="http://python.org/" target="_blank">Python.org</a></li>
            <li><a href="http://jinja.pocoo.org/" target="_blank">Jinja2</a></li>
            <li><a href="#" target="_blank">You can modify those links in your config file</a></li>
        </ul>
    </section>

</aside><aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="/logistic-classification-code.html">Logistic Classification Code</a>
      </li>
      <li class="post">
          <a href="/building-outward-toward-classification.html">Building outward toward Classification</a>
      </li>
      <li class="post">
          <a href="/linear-regression-code-pt-2.html">Linear Regression Code pt 2</a>
      </li>
      <li class="post">
          <a href="/linear-regression-code-pt-1.html">Linear Regression Code pt 1</a>
      </li>
      <li class="post">
          <a href="/linear-regression.html">Linear Regression</a>
      </li>
    </ul>
  </section>
  <section>
      
    <h1>Categories</h1>
    <ul id="recent_posts">
        <li><a href="/category/python-math.html">python, math</a></li>
    </ul>
  </section>
 

  <section>
  <h1>Tags</h1>
  </section>


    <section>
        <h1>Social</h1>
        <ul>
            <li><a href="/" type="application/rss+xml" rel="alternate">RSS</a></li>
            <li><a href="#" target="_blank">You can add links in your config file</a></li>
            <li><a href="#" target="_blank">Another social link</a></li>
        </ul>
    </section>
    <section>
        <h1>Blogroll</h1>
        <ul>
            <li><a href="http://getpelican.com/" target="_blank">Pelican</a></li>
            <li><a href="http://python.org/" target="_blank">Python.org</a></li>
            <li><a href="http://jinja.pocoo.org/" target="_blank">Jinja2</a></li>
            <li><a href="#" target="_blank">You can modify those links in your config file</a></li>
        </ul>
    </section>

</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2013 - Dan -
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
</body>
</html>